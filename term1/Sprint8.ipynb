{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint8 機械学習スクラッチ アンサンブル学習¶  \n",
    "\n",
    "#### 1.このSprintについて\n",
    "\n",
    "##### Sprintの目的\n",
    "アンサンブル学習について理解する ### どのように学ぶか スクラッチでアンサンブル学習の各種手法を実装していきます。\n",
    "\n",
    "#### 2.アンサンブル学習\n",
    "3種類のアンサンブル学習をスクラッチ実装していきます。そして、それぞれの効果を小さめのデータセットで確認します。\n",
    "\n",
    "・ブレンディング  \n",
    "・バギング  \n",
    "・スタッキング  \n",
    "\n",
    "##### 小さなデータセットの用意\n",
    "以前も利用した回帰のデータセットを用意します。\n",
    "\n",
    "House Prices: Advanced Regression Techniques\n",
    "\n",
    "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使います。　　\n",
    "train.csvを学習用（train）8割、検証用（val）2割に分割してください。\n",
    "\n",
    "##### scikit-learn\n",
    "単一のモデルはスクラッチ実装ではなく、scikit-learnなどのライブラリの使用を推奨します。\n",
    "\n",
    "sklearn.linear_model.LinearRegression — scikit-learn 0.21.3 documentation  \n",
    "sklearn.svm.SVR — scikit-learn 0.21.3 documentation  \n",
    "sklearn.tree.DecisionTreeRegressor — scikit-learn 0.21.3 documentation\n",
    "\n",
    "### 【問題1】ブレンディングのスクラッチ実装 \n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "##### ブレンディングとは\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。  \n",
    "最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "##### 手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）  \n",
    "入力データの前処理の仕方（例：標準化、対数変換、PCAなど）  \n",
    "重要なのはそれぞれのモデルが大きく異なることです。  \n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "##### 《補足》\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。\n",
    "\n",
    "sklearn.ensemble.VotingClassifier — scikit-learn 0.21.3 documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\anai\\dive\\Dataset\\House Prices\\train.csv\")\n",
    "X = df.drop(\"SalePrice\",axis=1)\n",
    "X = X.loc[:,[\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = df.loc[:,\"SalePrice\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=0)\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_train.shape\n",
    "X_test.shape\n",
    "#df_train = pd.concat([X_train, y_train], axis=1)\n",
    "#df_train.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class blending():\n",
    "    \"\"\"\n",
    "    ブレンディングを行う。\n",
    "    Parameters\n",
    "    ----------\n",
    "    kf_splits : int\n",
    "      Trainﾃﾞｰﾀを何分割するか\n",
    "    models : dict\n",
    "      ブレンディングに使うモデル。\n",
    "        ・keys:モデル名(str)\n",
    "        ・value:モデルを適用するclass(class)\n",
    "    \"\"\"\n",
    "    def __init__(self, kf_splits, models):\n",
    "        self.kf_splits = kf_splits\n",
    "        self.models = models\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        ブレンディングの学習を行う。\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : DataFrame\n",
    "          学習データ。(targetﾃﾞｰﾀ含む)\n",
    "        target_name : str\n",
    "          学習データの目標値のFutureの名前。\n",
    "        fits : dict\n",
    "          ブレンディングの推測に使う学習モデル。\n",
    "            ・keys :モデリング順につけた番号(int)\n",
    "            ・value:学習モデル(class)\n",
    "        n : int\n",
    "          モデリング順につける番号。fitsのkeyにつかう番号\n",
    "        Attributes\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        target_name = df_train.columns[-1]\n",
    "        self.fits = {}\n",
    "        n=1\n",
    "        kf = KFold(n_splits=self.kf_splits)\n",
    "        for train_index, test_index in kf.split(df_train):\n",
    "            X_train = df_train.iloc[train_index,:].drop([target_name], axis=1)\n",
    "            y_train = df_train.iloc[train_index,:].loc[:,[target_name]]\n",
    "            X_test = df_train.iloc[test_index,:].drop([target_name], axis=1)\n",
    "            y_test = df_train.iloc[test_index,:].loc[:,[target_name]]\n",
    "            \n",
    "            for model in self.models.values():\n",
    "                self.fits[n] = model.fit(X_train, y_train)\n",
    "                n += 1\n",
    "    def predict(self, X):\n",
    "        predicts = np.empty((X.shape[0], 1))\n",
    "        for fit in self.fits.values():\n",
    "            predicts = np.concatenate([predicts, fit.predict(X).reshape(X.shape[0], 1)], axis=1)\n",
    "            \n",
    "        predicts=predicts[:,1:]\n",
    "        predicts=np.mean(predicts,axis=1)\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(_X_train, _y_train, _X_test, _y_test, models):\n",
    "    _df_results = pd.DataFrame()\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(_X_train, _y_train)\n",
    "        _df_result = pd.DataFrame({model_name : [mean_squared_error(_y_test, model.predict(_X_test)),\n",
    "                                                 np.sqrt(mean_squared_error(_y_test, model.predict(_X_test))),\n",
    "                                                 #r2_score(_y_test, model.predict(_X_test))\n",
    "                                                ]}, \n",
    "                                    index = [\"mean_squared_error\", \"rmse\"])\n",
    "        _df_results = pd.concat([_df_results, _df_result],axis=1)       \n",
    "    return _df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        \"SVC\":SVC(gamma=\"auto\"),\n",
    "        \"DecisionTree\":DecisionTreeRegressor(random_state=0)\n",
    "        }\n",
    "models2 = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        \"SVC\":SVC(gamma=\"auto\"),\n",
    "        \"DecisionTree\":DecisionTreeRegressor(random_state=0),\n",
    "        \"LogisticRegression\":LogisticRegression(random_state=0),\n",
    "        }\n",
    "models3 = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        \"SVC\":SVC(gamma=\"auto\"),\n",
    "        \"DecisionTree\":DecisionTreeRegressor(random_state=0),\n",
    "        \"LogisticRegression\":LogisticRegression(random_state=0),\n",
    "        \"RandomForest\":RandomForestRegressor(random_state=0)\n",
    "        }\n",
    "\n",
    "blend_3split = blending(2, models3)\n",
    "blend_3split.fit(X_train, y_train)\n",
    "\n",
    "#blend_3split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:47: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>Blending1</th>\n",
       "      <th>Blending2</th>\n",
       "      <th>Blending3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>2.942067e+09</td>\n",
       "      <td>3.294162e+09</td>\n",
       "      <td>3.803085e+09</td>\n",
       "      <td>3.292721e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>5.424082e+04</td>\n",
       "      <td>5.739479e+04</td>\n",
       "      <td>6.166916e+04</td>\n",
       "      <td>5.738223e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    LinearRegression     Blending1     Blending2     Blending3\n",
       "mean_squared_error      2.942067e+09  3.294162e+09  3.803085e+09  3.292721e+09\n",
       "rmse                    5.424082e+04  5.739479e+04  6.166916e+04  5.738223e+04"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models2 = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        \"SVC\":SVC(gamma=\"auto\"),\n",
    "        \"DecisionTree\":DecisionTreeRegressor(random_state=0),\n",
    "        \"LogisticRegression\":LogisticRegression(random_state=0),\n",
    "        }\n",
    "models3 = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        \"SVC\":SVC(gamma=\"auto\"),\n",
    "        \"DecisionTree\":DecisionTreeRegressor(random_state=0),\n",
    "        \"LogisticRegression\":LogisticRegression(random_state=0),\n",
    "        \"RandomForest\":RandomForestRegressor(random_state=0)\n",
    "        }\n",
    "models_test = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        #\"SVC\":SVC(gamma=\"auto\"),\n",
    "        #\"LogisticRegression\":LogisticRegression(random_state=0),\n",
    "        #\"DecisionTree\":DecisionTreeRegressor(random_state=0),\n",
    "        \"Blending1\":blending(2, models),\n",
    "        \"Blending2\":blending(2, models2),\n",
    "        \"Blending3\":blending(2, models3)\n",
    "        }\n",
    "\n",
    "model_comparison(X_train, y_train, X_test, y_test, models_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ブレンディングで3条件でLinearRegressionより、精度が向上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】バギングのスクラッチ実装 \n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "##### バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。学習データから重複を許した上でランダムに抜き出すことで、  \n",
    "N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。  \n",
    "ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。  \n",
    "これによりブートストラップサンプルが手に入ります。  \n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bagging():\n",
    "    \"\"\"\n",
    "    ブレンディングを行う。\n",
    "    Parameters\n",
    "    ----------\n",
    "    kf_splits : int\n",
    "      Trainﾃﾞｰﾀを何分割するか\n",
    "    models : dict\n",
    "      ブレンディングに使うモデル。\n",
    "        ・keys:モデル名(str)\n",
    "        ・value:モデルを適用するclass(class)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter, models, test_size=0.2):\n",
    "        self.n_iter = n_iter\n",
    "        self.models = models\n",
    "        self.test_size = test_size\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        ブレンディングの学習を行う。\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : DataFrame\n",
    "          学習データ。(targetﾃﾞｰﾀ含む)\n",
    "        target_name : str\n",
    "          学習データの目標値のFutureの名前。\n",
    "        fits : dict\n",
    "          ブレンディングの推測に使う学習モデル。\n",
    "            ・keys :モデリング順につけた番号(int)\n",
    "            ・value:学習モデル(class)\n",
    "        n : int\n",
    "          モデリング順につける番号。fitsのkeyにつかう番号\n",
    "        Attributes\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        target_name = df_train.columns[-1]\n",
    "        self.fits = {}\n",
    "        col=1\n",
    "        for n in range(self.n_iter):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,\n",
    "                                                               test_size=self.test_size,\n",
    "                                                               shuffle = True)\n",
    "            for model in models.values():\n",
    "                self.fits[n] = model.fit(X_train, y_train)\n",
    "                col += 1\n",
    "                \n",
    "    def predict(self, X):\n",
    "        predicts = np.empty((X.shape[0], 1))\n",
    "        for fit in self.fits.values():\n",
    "            predicts = np.concatenate([predicts, fit.predict(X).reshape(X.shape[0], 1)], axis=1)\n",
    "            \n",
    "        predicts=predicts[:,1:]\n",
    "        predicts=np.mean(predicts,axis=1)\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([269177.74203601, 156950.46674618, 127085.86811902, 237896.4846376 ,\n",
       "       136161.56343839,  69343.88226209, 209685.19568538, 132115.16947079,\n",
       "       509440.52188756, 169435.87010491, 197162.18913427, 201663.07950008,\n",
       "       255371.63892783, 112460.24174757, 112657.99003596, 145368.22593906,\n",
       "       242346.80427261, 148119.20361614, 145296.2585792 , 157112.55525712,\n",
       "       147802.15626196, 143251.34297033,  99892.49852342, 198735.75016368,\n",
       "       219914.31599048, 116209.68556714, 214082.3495567 ,  87557.51556012,\n",
       "       233807.94902983, 125494.80110338, 193972.27381733, 228066.7515521 ,\n",
       "       124445.11261213, 278512.78049807, 270659.88630439, 197081.79268378,\n",
       "       217730.65191466, 115490.0119685 , 270722.77676865, 316359.94489025,\n",
       "       225535.56659744, 148156.15900353, 191855.39084861, 245102.96820246,\n",
       "       335068.27281173, 194886.45667948,  93003.71011772, 120635.3695482 ,\n",
       "       203926.49222141,  95034.3625782 , 329895.68454521, 128340.43837933,\n",
       "       166971.46625734,  75510.40203628, 210653.83992115, 113482.05174703,\n",
       "       111610.24495963, 242471.93739615, 131334.54882281,  82453.00401064,\n",
       "       124427.6066259 , 116969.56120401, 137327.30815758, 135507.37152391,\n",
       "       219147.31068591, 162274.77101808, 104598.91846326, 235143.5635456 ,\n",
       "        92848.10346951, 233507.75985691, 203455.13764195, 103348.88665076,\n",
       "       102340.04418973, 205046.2046576 , 107897.75652746, 227470.2638491 ,\n",
       "       117780.65537665,  79916.63280321, 310613.56115975, 150970.67494984,\n",
       "       128873.3878131 , 119496.85551583, 102847.70635161, 148119.20361614,\n",
       "       317293.57715356, 192684.63881246,  94022.27727932, 197834.53483996,\n",
       "       185660.99258263, 122222.54592102, 223364.87006006, 190735.67841242,\n",
       "       201465.33121169, 245726.0387873 , 187471.85232069, 142011.03585841,\n",
       "       208815.74949628, 144374.2944314 , 111680.26890457, 145687.21670816,\n",
       "       235564.99519923, 252991.52217358, 148048.53186622, 176455.62569191,\n",
       "        55798.41790656, 292295.54356226, 185491.12278604,  87428.49179375,\n",
       "       209792.82282268, 107580.70917329,  38462.01208328, 119896.24273543,\n",
       "       229044.47268348, 107575.52292051, 200631.54480004, 188055.37248526,\n",
       "       301443.2062415 , 122984.36497281, 241473.46744068, 259737.02366454,\n",
       "       167167.91893577, 202262.81004093, 115669.60646567, 224566.92236166,\n",
       "       214651.60657288, 213082.58399128, 276437.39136454, 191904.66596946,\n",
       "       185719.34459909, 185478.1552476 , 187543.81968055, 185616.25590958,\n",
       "       262323.96560277, 181807.16065063, 111680.26890457, 223158.69268105,\n",
       "       146798.50005372, 190990.48310731,  99861.37719378, 209358.42363062,\n",
       "       121828.3449542 , 167320.28274613, 238878.74421679, 136663.39154252,\n",
       "       151320.13924361, 199820.4506274 , 209891.37306439, 120416.22463076,\n",
       "       237420.59161035, 203984.19643289, 163650.5837591 , 235197.37711426,\n",
       "       244322.99535946, 181011.62523632, 150389.0982002 , 317427.13936774,\n",
       "       106961.52923128, 138937.17676941, 174222.68649524, 196884.69220037,\n",
       "       112657.34223098, 134333.1977141 , 220254.05558366, 121576.78309717,\n",
       "       220047.87820465, 129166.44350533, 166693.96932344, 289655.43586035,\n",
       "       195826.5746185 , 226143.72622892, 157263.62345752, 193613.08482299,\n",
       "       191443.03609058, 156433.7276887 , 108830.09318082, 137914.07116   ,\n",
       "       129743.48180717, 295993.76884605, 112712.45140959, 146649.37908121,\n",
       "       255098.68044173, 218587.7783703 ,  89011.12971876, 234426.48116686,\n",
       "        82058.80304382, 200341.08032771, 124360.17771383, 183554.48211946,\n",
       "       202316.62360958, 143251.34297033, 148119.20361614, 151755.18624064,\n",
       "       214073.27266109, 142238.60986648,  91100.1341957 , 139688.62331564,\n",
       "        87557.51556012, 128201.68991238, 191695.24575259, 164304.77567359,\n",
       "       148922.51650313, 166791.87176017,  73502.44181481, 137640.46486892,\n",
       "       168604.67491316, 293170.17600414, 145713.15178503, 273873.78947029,\n",
       "       237120.40243743,  56964.16262574, 170672.93437898, 222226.35602769,\n",
       "       110282.41172951,  58487.80072932, 223248.81383212, 263077.35556393,\n",
       "       174909.29534932, 319676.93674566, 228501.15074416,  89511.66221294,\n",
       "       184128.92538842, 164156.95031103, 132537.24892939, 113634.41555739,\n",
       "       212060.77399182, 265774.51967235, 251503.54384745, 213334.79365329,\n",
       "       141235.60146321, 169705.58575316, 109305.3384031 , 106396.81066289,\n",
       "       112657.34223098, 160332.94409872, 236775.47659147, 137883.59763533,\n",
       "       211254.86607196,  92851.34630736, 126713.71158624, 165205.9909973 ,\n",
       "       132504.8319898 , 223620.32255992, 236963.50017927, 237017.31374793,\n",
       "       158572.00728649, 150405.30857648, 176491.28546935, 218918.44106788,\n",
       "       128139.4472531 , 198466.68232041, 160154.6452115 , 257675.24987441,\n",
       "        64780.74923701, 361382.47151765, 196176.03891226, 245102.96820246,\n",
       "        87459.61312339, 220509.50808353, 392882.8738335 , 641366.81376868,\n",
       "       230217.99868832, 148963.36253335, 192022.66561232,  87613.2725437 ,\n",
       "       273600.18317922, 242772.12656906, 303317.60424879,  34203.60667891,\n",
       "       201444.58238761, 234372.66759821, 130389.892436  , 194044.88898217,\n",
       "       148447.91908582, 194903.96266572, 195726.07714889, 270668.9632    ,\n",
       "       220810.34506142, 172673.76111977, 256648.90142715, 111484.46403112,\n",
       "       116727.07624257, 259947.73949135, 149268.09015406, 158796.98626168,\n",
       "       191183.04514291, 224244.68875471, 150127.81164258, 222293.78493975,\n",
       "       307359.4597686 , 230251.71123786,  66286.23354938,  88289.50889222])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        #\"SVC\":SVC(gamma=\"auto\"),\n",
    "        #\"DecisionTree\":DecisionTreeRegressor(random_state=0)\n",
    "        }\n",
    "bagging1 = bagging(3, models)\n",
    "bagging1.fit(X_train, y_train)\n",
    "bagging1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>bagging5</th>\n",
       "      <th>bagging10</th>\n",
       "      <th>bagging15</th>\n",
       "      <th>bagging20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>2.942067e+09</td>\n",
       "      <td>2.920615e+09</td>\n",
       "      <td>3.194797e+09</td>\n",
       "      <td>3.591319e+09</td>\n",
       "      <td>7.722125e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>5.424082e+04</td>\n",
       "      <td>5.404272e+04</td>\n",
       "      <td>5.652253e+04</td>\n",
       "      <td>5.992762e+04</td>\n",
       "      <td>8.787562e+04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    LinearRegression      bagging5     bagging10  \\\n",
       "mean_squared_error      2.942067e+09  2.920615e+09  3.194797e+09   \n",
       "rmse                    5.424082e+04  5.404272e+04  5.652253e+04   \n",
       "\n",
       "                       bagging15     bagging20  \n",
       "mean_squared_error  3.591319e+09  7.722125e+09  \n",
       "rmse                5.992762e+04  8.787562e+04  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_test_bag = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        #\"SVC\":SVC(gamma=\"auto\"),\n",
    "        #\"LogisticRegression\":LogisticRegression(random_state=0),\n",
    "        #\"DecisionTree\":DecisionTreeRegressor(random_state=0),\n",
    "        \"bagging5\":bagging(5, models),\n",
    "        \"bagging10\":bagging(10, models),\n",
    "        \"bagging15\":bagging(25, models),\n",
    "        \"bagging20\":bagging(20, models)\n",
    "        }\n",
    "\n",
    "model_comparison(X_train, y_train, X_test, y_test, models_test_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バギングでLinearRegressionより、精度が向上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】スタッキングのスクラッチ実装 \n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "##### スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。  \n",
    "まずは $K_0 = 3, M_0 = 2$ 程度にします。\n",
    "\n",
    "##### 《学習時》\n",
    "（ステージ 0 ）  \n",
    "学習データを $K_0$ 個に分割する。  \n",
    "分割した内の $(K_0 − 1)$ 個をまとめて学習用データ、残り 1 個を推定用データとする組み合わせが $K_0$ 個作れる。  \n",
    "あるモデルのインスタンスを $K_0$ 個用意し、異なる学習用データを使い学習する。  \n",
    "それぞれの学習済みモデルに対して、使っていない残り 1 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）  \n",
    "さらに、異なるモデルのインスタンスも $K_0$ 個用意し、同様のことを行う。モデルが $M_0$ 個あれば、$M_0$ 個のブレンドデータが得られる。\n",
    "\n",
    "（ステージ n ）  \n",
    "ステージ $n −1 $ のブレンドデータを$M_n − 1$ 次元の特徴量を持つ学習用データと考え、$K_n$ 個に分割する。以下同様である。  \n",
    "（ステージ N ）＊最後のステージ  \n",
    "ステージ$N − 1$ の$M_{N-1}$ 個のブレンドデータを$M_{N−1}$ 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "##### 《推定時》  \n",
    "（ステージ 0 ）  \n",
    "テストデータを $K_0×M_0$ 個の学習済みモデルに入力し、$K_0×M_0$ 個の推定値を得る。  \n",
    "これを$K_0$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ n ）  \n",
    "ステージ $n−1$ で得たブレンドテストを$K_n×M_n$ 個の学習済みモデルに入力し、$K_n×M_n$ 個の推定値を得る。  \n",
    "これを_\\K_n$ の軸で平均値を求め $M_0 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ N ）＊最後のステージ  \n",
    "ステージ$N−1$ で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stucking():\n",
    "    \"\"\"\n",
    "    ブレンディングを行う。\n",
    "    Parameters\n",
    "    ----------\n",
    "    kf_splits : int\n",
    "      Trainﾃﾞｰﾀを何分割するか\n",
    "    models : dict\n",
    "      ブレンディングに使うモデル。\n",
    "        ・keys:モデル名(str)\n",
    "        ・value:モデルを適用するclass(class)\n",
    "    \"\"\"\n",
    "    def __init__(self, split, n_iter, models):\n",
    "        self.split = split\n",
    "        self.models = models\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        ブレンディングの学習を行う。\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : DataFrame\n",
    "          学習データ。(targetﾃﾞｰﾀ含む)\n",
    "        target_name : str\n",
    "          学習データの目標値のFutureの名前。\n",
    "        fits : dict\n",
    "          ブレンディングの推測に使う学習モデル。\n",
    "            ・keys :モデリング順につけた番号(int)\n",
    "            ・value:学習モデル(class)\n",
    "        n : int\n",
    "          モデリング順につける番号。fitsのkeyにつかう番号\n",
    "        Attributes\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        target_name = df_train.columns[-1]\n",
    "        \n",
    "        n_block = round(X_train.shape[0]/self.split)\n",
    "        if X_train.shape[0]%n_block is not 0:\n",
    "            if X_train.shape[0]%n_block < n_block/2:\n",
    "                n_block_0 = n_block + X_train.shape[0]%n_block\n",
    "            else:\n",
    "                n_block_0 = X_train.shape[0]%n_block\n",
    "        else:\n",
    "            n_block_0 = n_block\n",
    "        \n",
    "        self.X_train_b = X_train\n",
    "        self.fits = [[] for i in range(3)]\n",
    "        for stage in range(self.n_iter):            \n",
    "            self.predicts_t = np.empty((X_train.shape[0], 1))  \n",
    "            for model in self.models.values():\n",
    "                i0=0\n",
    "                self.predicts_k = 0  \n",
    "                for k0 in range(self.split):\n",
    "                    if k0==0:\n",
    "                        i=n_block_0\n",
    "                    else:\n",
    "                        i=i0+n_block\n",
    "\n",
    "                    X_test_k = self.X_train_b.iloc[i0:i]\n",
    "                    y_test_k = y_train.iloc[i0:i]\n",
    "                    X_train_k = self.X_train_b.drop(X_test_k.index, axis=0)\n",
    "                    y_train_k = y_train.drop(y_test_k.index, axis=0)\n",
    "\n",
    "                    fit_model = model.fit(X_train_k, y_train_k)\n",
    "                    self.fits[2].append(stage)\n",
    "                    self.fits[1].append(model)\n",
    "                    self.fits[0].append(fit_model)\n",
    "                    self.predicts_k = np.append(self.predicts_k, fit_model.predict(X_test_k))\n",
    "                    i0=i\n",
    "                    \n",
    "                self.predicts_k = np.delete(self.predicts_k, 0)\n",
    "                self.predicts_t = np.concatenate([self.predicts_t, self.predicts_k.reshape(X_train.shape[0], 1)], axis=1)\n",
    "            \n",
    "            self.predicts_t =np.delete(self.predicts_t, 0, 1)\n",
    "            self.X_train_b = pd.DataFrame(self.predicts_t)\n",
    "            \n",
    "        self.last_model = list(self.models.values())[0].fit(self.X_train_b, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X_blend_test = X\n",
    "        self.predict_main = [[] for i in range(3)]\n",
    "        for stage in range(self.n_iter):\n",
    "            self.blend_test1 = np.empty((X_train.shape[0], 1))\n",
    "            for model in range(len(list(self.models.values()))):\n",
    "                self.blend_test_a = np.zeros((1,1))\n",
    "                for k0 in range(self.split):\n",
    "                    display(\"test00:{}\".format(self.blend_test_a.shape))\n",
    "                    self.blend_test_a = np.append(self.blend_test_a, self.fits[0][k0].predict(self.X_blend_test))\n",
    "                    #display(\"test0_1{}\".format(self.blend_test_a.shape))\n",
    "                self.blend_test_a = np.delete(self.blend_test_a, 0)\n",
    "                self.blend_test1 = np.concatenate([self.blend_test1, self.blend_test_a.reshape(self.blend_test_a.shape[0], 1)], axis=1)               \n",
    "\n",
    "            self.blend_test1 =np.delete(self.blend_test1, 0, 1)\n",
    "            self.X_blend_test = pd.DataFrame(self.blend_test1)\n",
    "            \n",
    "        self.last_predict = self.last_model.predict(self.X_blend_test)\n",
    "        return self.last_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stucking():\n",
    "    \"\"\"\n",
    "    ブレンディングを行う。\n",
    "    Parameters\n",
    "    ----------\n",
    "    kf_splits : int\n",
    "      Trainﾃﾞｰﾀを何分割するか\n",
    "    models : dict\n",
    "      ブレンディングに使うモデル。\n",
    "        ・keys:モデル名(str)\n",
    "        ・value:モデルを適用するclass(class)\n",
    "    \"\"\"\n",
    "    def __init__(self, split, n_iter, models):\n",
    "        self.split = split\n",
    "        self.models = models\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        ブレンディングの学習を行う。\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : DataFrame\n",
    "          学習データ。(targetﾃﾞｰﾀ含む)\n",
    "        target_name : str\n",
    "          学習データの目標値のFutureの名前。\n",
    "        fits : dict\n",
    "          ブレンディングの推測に使う学習モデル。\n",
    "            ・keys :モデリング順につけた番号(int)\n",
    "            ・value:学習モデル(class)\n",
    "        n : int\n",
    "          モデリング順につける番号。fitsのkeyにつかう番号\n",
    "        Attributes\n",
    "        ----------\n",
    "        \"\"\"\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        target_name = df_train.columns[-1]\n",
    "        \n",
    "        n_block = round(X_train.shape[0]/self.split)\n",
    "        if X_train.shape[0]%n_block is not 0:\n",
    "            if X_train.shape[0]%n_block < n_block/2:\n",
    "                n_block_0 = n_block + X_train.shape[0]%n_block\n",
    "            else:\n",
    "                n_block_0 = X_train.shape[0]%n_block\n",
    "        else:\n",
    "            n_block_0 = n_block\n",
    "        \n",
    "        self.X_train_b = X_train\n",
    "        self.fits = [[] for i in range(3)]\n",
    "        for stage in range(self.n_iter):            \n",
    "            self.predicts_t = np.empty((X_train.shape[0], 1))  \n",
    "            for model in self.models.values():\n",
    "                i0=0\n",
    "                self.predicts_k = 0  \n",
    "                for k0 in range(self.split):\n",
    "                    if k0==0:\n",
    "                        i=n_block_0\n",
    "                    else:\n",
    "                        i=i0+n_block\n",
    "\n",
    "                    X_test_k = self.X_train_b.iloc[i0:i]\n",
    "                    y_test_k = y_train.iloc[i0:i]\n",
    "                    X_train_k = self.X_train_b.drop(X_test_k.index, axis=0)\n",
    "                    y_train_k = y_train.drop(y_test_k.index, axis=0)\n",
    "\n",
    "                    fit_model = model.fit(X_train_k, y_train_k)\n",
    "                    self.fits[2].append(stage)\n",
    "                    self.fits[1].append(model)\n",
    "                    self.fits[0].append(fit_model)\n",
    "                    self.predicts_k = np.append(self.predicts_k, fit_model.predict(X_test_k))\n",
    "                    i0=i\n",
    "                    \n",
    "                self.predicts_k = np.delete(self.predicts_k, 0)\n",
    "                self.predicts_t = np.concatenate([self.predicts_t, self.predicts_k.reshape(X_train.shape[0], 1)], axis=1)\n",
    "            \n",
    "            self.predicts_t =np.delete(self.predicts_t, 0, 1)\n",
    "            self.X_train_b = pd.DataFrame(self.predicts_t)\n",
    "            \n",
    "        self.last_model = list(self.models.values())[0].fit(self.X_train_b, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X_blend_test = X\n",
    "        self.predict_main = [[] for i in range(3)]\n",
    "        for stage in range(self.n_iter):\n",
    "            self.blend_test1 = np.empty((self.X_blend_test.shape[0], 1))\n",
    "            for model in range(len(list(self.models.values()))):\n",
    "                blend = np.empty((self.X_blend_test.shape[0], 1))\n",
    "                for k0 in range(self.split):\n",
    "                    #display(\"brend:{}\".format(blend.shape))\n",
    "                    #display(self.fits[0][k0].predict(self.X_blend_test).shape)\n",
    "                    blend = np.concatenate([blend, self.fits[0][k0].predict(self.X_blend_test).reshape(-1,1)], axis=1)\n",
    "                blend = np.delete(blend, 0, 1)\n",
    "                blend = np.mean(blend, axis=1)\n",
    "                self.blend_test1 = np.concatenate([self.blend_test1, blend.reshape(blend.shape[0], 1)], axis=1)               \n",
    "\n",
    "            self.blend_test1 =np.delete(self.blend_test1, 0, 1)\n",
    "            self.X_blend_test = pd.DataFrame(self.blend_test1)\n",
    "        self.last_predict = self.last_model.predict(self.X_blend_test)\n",
    "        return self.last_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1168,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        #\"SVC\":SVC(gamma=\"auto\"),\n",
    "        \"DecisionTree\":DecisionTreeRegressor(random_state=0)\n",
    "        }\n",
    "\n",
    "stucking_0 = stucking(5, 3, models)\n",
    "stucking_0.fit(X_train, y_train)\n",
    "#stucking_0.fits\n",
    "stucking_0.predicts_k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15358.18275311, 14506.13408846, 14165.39243358, 14910.35875767,\n",
       "       14064.84202106, 14000.18022182, 14403.61516106, 14321.52970407,\n",
       "       17141.48535632, 14074.4888415 , 14301.78481269, 14599.44713542,\n",
       "       14875.27673285, 13890.23717586, 14055.36378556, 14266.4148839 ,\n",
       "       14820.61578239, 13972.50676668, 14188.38839489, 14731.13992381,\n",
       "       14330.87307163, 13843.59039122, 13813.80962448, 14418.84983394,\n",
       "       14777.58702573, 14654.22347614, 14502.58349445, 13644.8445168 ,\n",
       "       14616.332631  , 14177.15949532, 14686.41238816, 14686.0933864 ,\n",
       "       13901.33120749, 15070.73912996, 14932.69854735, 14387.06683534,\n",
       "       14491.75919871, 13873.95858602, 14976.24860769, 15456.07476214,\n",
       "       15033.53424465, 14308.19742169, 14263.58789549, 14741.73081984,\n",
       "       15612.44805153, 14642.87918631, 13821.85619769, 13884.9346082 ,\n",
       "       14602.20799946, 13917.15461802, 15498.04274335, 14047.46893879,\n",
       "       14284.93303409, 13787.98536516, 14566.94053122, 13864.85123736,\n",
       "       14372.89031872, 14709.93096275, 14091.06957461, 13789.01242363,\n",
       "       14030.16329048, 14024.59098693, 14195.50906059, 14119.26695262,\n",
       "       14598.84153927, 14415.49892266, 13915.50510491, 14610.91205389,\n",
       "       14207.44339766, 14648.08191262, 14374.47501705, 14050.66681404,\n",
       "       13929.98245521, 14362.70795531, 13944.59467345, 14569.04522363,\n",
       "       13980.14873606, 13921.43012722, 15310.81236287, 14255.62430515,\n",
       "       13923.18210087, 13857.69685468, 14295.62115229, 13972.50676668,\n",
       "       15877.06429829, 14288.09850196, 14174.80192552, 14316.31273846,\n",
       "       14174.56852581, 14053.71427245, 14601.6361203 , 14503.08793964,\n",
       "       14434.32052572, 14764.42332828, 14216.33420511, 14211.01346935,\n",
       "       14421.74556592, 14593.01504874, 13857.56198673, 14501.40339995,\n",
       "       14649.02467862, 14819.8921768 , 14290.05015834, 14368.56554364,\n",
       "       13725.95087161, 15269.66913821, 14310.65614245, 13936.07606244,\n",
       "       14421.76242442, 14302.9609784 , 13702.06271978, 14007.40334808,\n",
       "       14883.89518522, 14087.93782373, 14392.57170492, 14479.4526652 ,\n",
       "       15247.14521471, 14017.43660425, 15019.29291325, 14853.5775659 ,\n",
       "       14054.48976313, 14338.16363185, 13970.13233839, 14870.20887449,\n",
       "       14516.20237121, 14416.37556429, 15000.84350663, 14255.42331283,\n",
       "       14200.88037182, 14456.72643978, 14294.36069412, 14199.9713228 ,\n",
       "       15108.56516032, 14381.35966385, 13857.56198673, 14599.81802226,\n",
       "       14425.21186747, 14298.95651468, 13890.92706447, 14529.72009701,\n",
       "       14119.03093372, 14047.23422948, 15125.39877083, 14017.67262315,\n",
       "       14215.71044087, 14437.01395579, 14405.4332591 , 14029.18680749,\n",
       "       14665.38756092, 14430.73490513, 14367.43733422, 14619.98568556,\n",
       "       14709.05563072, 15070.87137871, 14585.86066605, 15603.07096698,\n",
       "       14099.72174396, 14450.47979651, 14090.90229928, 14419.72516597,\n",
       "       13857.57884523, 14151.90842476, 14505.41179246, 13944.83069235,\n",
       "       14503.59369442, 14450.31121159, 14402.87338738, 15203.39285245,\n",
       "       14307.2053898 , 14608.94222942, 14328.31450951, 14494.06488344,\n",
       "       14259.95169942, 14106.01896271, 13970.01432894, 14080.29585435,\n",
       "       14102.83663635, 15448.46520015, 14262.22235757, 14054.13573478,\n",
       "       15010.45530048, 14817.48403152, 14087.61751237, 14621.7869251 ,\n",
       "       13854.3290849 , 14656.58235554, 13969.3750158 , 14181.79034247,\n",
       "       14347.23726353, 13843.59039122, 13972.50676668, 14305.53770861,\n",
       "       14468.10706578, 13903.45275839, 14209.22777871, 13984.15581897,\n",
       "       13644.8445168 , 14106.43911543, 14631.93688113, 14313.01240266,\n",
       "       14289.15796781, 14188.75928173, 13778.8780165 , 14017.68948164,\n",
       "       14823.87978202, 15466.56188802, 14209.26280529, 15029.83192421,\n",
       "       14697.13684254, 13856.61791114, 14085.3974297 , 14574.39836678,\n",
       "       14017.21744384, 13784.06257472, 14746.79736861, 15235.59600378,\n",
       "       14354.92980839, 15554.11718801, 14578.1357138 , 13644.87823378,\n",
       "       14410.43237389, 14337.50615063, 14557.42726914, 13857.59570372,\n",
       "       14441.76150279, 14932.61425489, 14832.56828756, 14788.36074599,\n",
       "       14195.57649456, 14317.64194021, 14017.20058535, 14103.34108154,\n",
       "       13857.57884523, 14071.60996801, 14754.28892115, 14355.19823468,\n",
       "       14701.22690832, 13829.11173133, 14119.11522619, 14415.54949814,\n",
       "       14238.97482847, 14595.28963567, 14687.15416185, 14696.22779352,\n",
       "       14219.46464639, 14061.45870239, 14308.68631798, 14510.83236957,\n",
       "       14260.67399542, 14373.48167557, 14371.00609631, 14835.39658556,\n",
       "       13856.75277908, 15749.89989649, 14267.29152553, 14741.73081984,\n",
       "       13858.95862245, 14499.06530783, 16053.47169263, 18201.63881185,\n",
       "       14653.46877275, 14444.30189682, 14703.61688551, 14247.27296947,\n",
       "       14967.2255515 , 14678.18168113, 15530.24589468, 13544.12420976,\n",
       "       14941.48427505, 14612.71329343, 14409.50515679, 14962.22381751,\n",
       "       14439.75665173, 14514.04710332, 15097.43610212, 14967.17497602,\n",
       "       14665.10096655, 14653.38317069, 14843.54430973, 14285.79019803,\n",
       "       13884.86717423, 14872.63387827, 14429.79082953, 14488.02054209,\n",
       "       14249.05996971, 15013.55202477, 14179.39905568, 14635.18664146,\n",
       "       15256.31999734, 15367.49109408, 13715.24458533, 14081.25416925])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stucking_0.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>stucking5</th>\n",
       "      <th>stucking10</th>\n",
       "      <th>stucking15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>2.942067e+09</td>\n",
       "      <td>3.471547e+10</td>\n",
       "      <td>3.735432e+10</td>\n",
       "      <td>3.639037e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>5.424082e+04</td>\n",
       "      <td>1.863209e+05</td>\n",
       "      <td>1.932727e+05</td>\n",
       "      <td>1.907626e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    LinearRegression     stucking5    stucking10    stucking15\n",
       "mean_squared_error      2.942067e+09  3.471547e+10  3.735432e+10  3.639037e+10\n",
       "rmse                    5.424082e+04  1.863209e+05  1.932727e+05  1.907626e+05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_test_bag = {\n",
    "        \"LinearRegression\" : LinearRegression(),\n",
    "        #\"SVC\":SVC(gamma=\"auto\"),\n",
    "        #\"LogisticRegression\":LogisticRegression(random_state=0),\n",
    "        #\"DecisionTree\":DecisionTreeRegressor(random_state=0),\n",
    "        \"stucking5\":stucking(5, 3, models),\n",
    "        \"stucking10\":stucking(10, 3, models),\n",
    "        \"stucking15\":stucking(15, 3, models),\n",
    "        }\n",
    "\n",
    "model_comparison(X_train, y_train, X_test, y_test, models_test_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スタッキングでLinearRegressionより、精度が向上"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
