{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3063e348c500d27ff6688fdfa25e96a5eb935e21"
   },
   "source": [
    "## Keras - Clean Project Workflow\n",
    "\n",
    "\n",
    "### Aim:\n",
    "\n",
    "\n",
    "Aim of this notebook is to show an example of clean workflow Computer Vision project/competition in Keras. \n",
    "\n",
    "### Workflow:\n",
    "\n",
    "\n",
    "Workflow can be interpreted as following steps:\n",
    "1. Dataset initialization (if needed): this step is usually required in case where training samples are separated from from their labels or there is additional information about the samples of a different format. This is the case here, __depth__ is an additional feature that is separate from training images and is thus provided in DataFrame format. For easy integration between the depth and images, each sample has a unique ID. By those IDs images can be connected with their masks and depth added on top of that.\n",
    "2. Data loading/processing: set of operations preparing the data for model-ingestible format. Each sample is loaded as image and appended to a list, same happends with masks. Afterwards, dimensions are expanded (if needed), because 2D Convolutional CNN require input samples of dimensionality (HxWxC - height x width x channels) and OpenCV loads grayscale images as (HxW) 2D arrays.\n",
    "3. Data is normalized to 0-1 input range. When loaded in OpenCV, grayscale images come in range between 0 and 255. Networks usually converge quicker if data is in 0-1 range. It is also important to keep the values range the same for images and masks (feeding the model with 0-255 images and 0-1 masks is not recommended).\n",
    "4. Data is split into training and validation subsets. For this competition, salt coverage is the basis of the split. Then, a stratified split is performed in order to avoid significant discrepancy in distribution between training and validation sets. This could potentially harm model performance or at least skew the validation metric results.\n",
    "5. Model definition and training. A lot more about this can be read either in segmentation papers, solutions from past competition or discussions part itself :). One major principle to keep in mind - segmentation model output must be of the same shape as was the input! \n",
    "6. Prediction with trained model.\n",
    "7. Predictions processing. This can be done in different ways, depending on the final goal. For this competition, predictions and encoded with Run Length Encoding in order to compress their size (raw masks predictions would weight around a GB). Method of processing is very important, as it may require a specific approach to final predictions preparation. In case of RLE, one have to make sure that predictions are scaled (or unpadded) to original image size. Otherwise, RLE will encode wrong pixels and thus final submission score will be low. \n",
    "\n",
    "Keras-プロジェクトワークフローのクリーン\n",
    "目的：\n",
    "このノートブックの目的は、KerasでのクリーンなワークフローComputer Visionプロジェクト/競争の例を示すことです。\n",
    "\n",
    "ワークフロー：\n",
    "ワークフローは、次の手順として解釈できます。\n",
    "\n",
    "データセットの初期化（必要な場合）：このステップは通常、トレーニングサンプルがラベルから分離されている場合、または異なる形式のサンプルに関する追加情報がある場合に必要です。これはこの場合です。深度はトレーニング画像とは別の追加機能であり、DataFrame形式で提供されます。深度と画像を簡単に統合するために、各サンプルには一意のIDがあります。これらのIDにより、画像はマスクとその上に追加された深度で接続できます。\n",
    "データのロード/処理：モデルで取り込み可能な形式のデータを準備する一連の操作。各サンプルは画像としてロードされ、リストに追加されます。マスクで同じことが起こります。その後、2D畳み込みCNNは次元の入力サンプル（HxWxC-高さx幅xチャンネル）を必要とし、OpenCVはグレースケール画像を（HxW）2D配列としてロードするため、次元は拡張されます（必要な場合）。\n",
    "データは0〜1の入力範囲に正規化されます。OpenCVに読み込まれると、グレースケール画像の範囲は0〜255になります。データが0〜1の範囲にある場合、ネットワークは通常より速く収束します。画像とマスクの値の範囲を同じにすることも重要です（モデルに0〜255の画像と0〜1のマスクを供給することはお勧めしません）。\n",
    "データは、トレーニングと検証のサブセットに分割されます。このコンペティションでは、ソルトカバレッジがスプリットの基礎となります。次に、トレーニングセットと検証セットの間の分布の大きな不一致を回避するために、階層化された分割が実行されます。これにより、モデルのパフォーマンスが低下したり、少なくとも検証メトリックの結果が歪んだりする可能性があります。\n",
    "モデルの定義とトレーニング。これについての詳細は、セグメンテーションペーパー、過去の競合からのソリューション、または議論自体で読むことができます:)。念頭に置いておくべき1つの主要な原則-セグメンテーションモデルの出力は、入力と同じ形状でなければなりません！\n",
    "訓練されたモデルによる予測。\n",
    "予測処理。これは、最終目標に応じて、さまざまな方法で実行できます。この競争では、予測を実行し、サイズを圧縮するためにRun Length Encodingでエンコードされます（rawマスクの予測は約1 GBになります）。最終的な予測の準備に特定のアプローチが必要になる場合があるため、処理方法は非常に重要です。RLEの場合、予測が元の画像サイズに合わせてスケーリングされる（またはパディングされない）ことを確認する必要があります。そうしないと、RLEが間違ったピクセルをエンコードするため、最終的な提出スコアが低くなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.callbacks import *\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## Define SaltParser\n",
    "\n",
    "The first question most probably would be - __why create and use parser like this one?__\n",
    "\n",
    "In Machine Learning, you usually can tune two things: models and data. Each parameter can influence the final score, so it's good to know what kind of parameters are used for each run and it's even better to design the pipeline in a way that will minimize potential errors. \n",
    "\n",
    "When a certain operation will be used many times but with different parameters, it is good to parameterize it and just call with chosen parameters. Besides, having functions for processing in one place makes it easier to spot mistakes. This is even more important when you perform an operation in different parts of the pipeline. Then, making sure that all functions are doing the same (for example using different types of padding for training and prediction certainly would not be a good idea!).\n",
    "\n",
    "SaltParserの定義\n",
    "最初の質問はおそらくおそらく- なぜこのようなパーサーを作成して使用するのでしょうか？\n",
    "\n",
    "機械学習では、通常、モデルとデータの2つのことを調整できます。各パラメーターは最終スコアに影響を与える可能性があるため、実行ごとにどのようなパラメーターが使用されているかを把握しておくとよいでしょう。\n",
    "\n",
    "特定の操作が何度も使用され、パラメーターが異なる場合は、それをパラメーター化して、選択したパラメーターを使用して呼び出すことをお勧めします。また、1つの場所で処理する機能があると、間違いを見つけやすくなります。パイプラインのさまざまな部分で操作を実行する場合、これはさらに重要です。そして、すべての機能が同じことをしていることを確認してください（たとえば、トレーニングと予測に異なるタイプのパディングを使用することは、確かに良い考えではありません！）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d271494cfd45b33e90711cd1ef9a466811d3152"
   },
   "outputs": [],
   "source": [
    "class SaltParser(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Parser for Salt Competition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_src='../input/',\n",
    "                 image_size=(128, 128),\n",
    "                 pad_images=False,\n",
    "                 grayscale=True,\n",
    "                 load_test_data=True):\n",
    "\n",
    "        self.data_src = data_src\n",
    "        self.image_size = image_size\n",
    "        self.pad_images = pad_images\n",
    "        self.grayscale = grayscale\n",
    "        self.load_test_data = load_test_data\n",
    "\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.padding_pixels = None\n",
    "\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.X_test = []\n",
    "\n",
    "        self.orig_image_size = (101, 101)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Arguments:\n",
    "        \n",
    "            data_src: directory containing data\n",
    "            image_size: tuple specifying final image size\n",
    "            pad_images: whether images should be padded or resized\n",
    "            grayscale: whether to load images as grayscale\n",
    "            load_test_data: whether to load test data\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "    def initialize_data(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize processing by loading .csv files.\n",
    "        \"\"\"\n",
    "\n",
    "        train_df = pd.read_csv('{}train.csv'.format(self.data_src),\n",
    "                               usecols=[0], index_col='id')\n",
    "        depths_df = pd.read_csv('{}depths.csv'.format(self.data_src),\n",
    "                                index_col='id')\n",
    "\n",
    "        self.train_df = train_df.join(depths_df)\n",
    "        self.test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "\n",
    "        return\n",
    "\n",
    "    def load_data(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Load images and masks from training set.\n",
    "        \n",
    "        # Returns:\n",
    "            self.X_train: np.array of training images\n",
    "            self.y_train: np.array of training masks\n",
    "            self.X_test: np.array of test images\n",
    "        \"\"\"\n",
    "\n",
    "        print('Loading training set.')\n",
    "        # Loop over ids in train_df\n",
    "        for i in tqdm(self.train_df.index):\n",
    "            # Load image and mask according to ID\n",
    "            img_src = '{}train/images/{}.png'.format(self.data_src, i)\n",
    "            mask_src = '{}train/masks/{}.png'.format(self.data_src, i)\n",
    "            # Specify if image should be loaded in grayscale.\n",
    "            if self.grayscale:\n",
    "                img_temp = cv2.imread(img_src, 0)\n",
    "            else:\n",
    "                img_temp = cv2.imread(img_src)\n",
    "            # Load mask\n",
    "            mask_temp = cv2.imread(mask_src, 0)\n",
    "            # Resize or pad image and mask\n",
    "            if self.orig_image_size != self.image_size:\n",
    "                if self.pad_images:\n",
    "                    img_temp = self.__pad_image(img_temp)\n",
    "                    mask_temp = self.__pad_image(mask_temp)\n",
    "                else:\n",
    "                    img_temp = cv2.resize(img_temp, self.image_size)\n",
    "                    mask_temp = cv2.resize(mask_temp, self.image_size)\n",
    "            # Append processed image and mask\n",
    "            self.X_train.append(img_temp)\n",
    "            self.y_train.append(mask_temp)\n",
    "\n",
    "        # Transform into arrays\n",
    "        self.X_train = np.asarray(self.X_train)\n",
    "        self.y_train = np.asarray(self.y_train)\n",
    "        # If images were loaded as grayscale, they are loaded as (HxW) arrays\n",
    "        # Dimensions must be expanded for the model to be trained.\n",
    "        if self.grayscale:\n",
    "            self.X_train = np.expand_dims(self.X_train, -1)\n",
    "        # Mask must be expanded obligatorily, as they are 1-channel by default.\n",
    "        self.y_train = np.expand_dims(self.y_train, -1)\n",
    "\n",
    "        # Output information about training set.\n",
    "        print('Training set ready.')\n",
    "        print('X_train shape: {}'.format(self.X_train.shape))\n",
    "        print('y_train shape: {}'.format(self.y_train.shape))\n",
    "        print('X_train - min: {}, max: {}'.format(\n",
    "            np.min(self.X_train), np.max(self.X_train)))\n",
    "        print('y_train - min: {}, max: {}'.format(\n",
    "            np.min(self.y_train), np.max(self.y_train)))\n",
    "\n",
    "        # Load test data.\n",
    "        # Perform similar steps to the training processing part,\n",
    "        # but there are no masks to be loaded.\n",
    "        if self.load_test_data:\n",
    "            print('Loading test set.')\n",
    "            for i in tqdm(self.test_df.index):\n",
    "                img_src = '{}test/images/{}.png'.format(self.data_src, i)\n",
    "                if self.grayscale:\n",
    "                    img_temp = cv2.imread(img_src, 0)\n",
    "                else:\n",
    "                    img_temp = cv2.imread(img_src)\n",
    "                if self.orig_image_size != self.image_size:\n",
    "                    if self.pad_images:\n",
    "                        img_temp = self.__pad_image(img_temp)\n",
    "                    else:\n",
    "                        img_temp = cv2.resize(img_temp, self.image_size)\n",
    "                self.X_test.append(img_temp)\n",
    "\n",
    "            self.X_test = np.asarray(self.X_test)\n",
    "            if self.grayscale:\n",
    "                self.X_test = np.expand_dims(self.X_test, -1)\n",
    "\n",
    "            print('Test set ready.')\n",
    "            print('X_test shape: {}'.format(self.X_test.shape))\n",
    "            print('X_test - min: {}, max: {}'.format(\n",
    "                np.min(self.X_test), np.max(self.X_test)))\n",
    "\n",
    "            return self.X_train, self.y_train, self.X_test\n",
    "\n",
    "        return self.X_train, self.y_train\n",
    "\n",
    "    def compute_coverage(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute salt coverage of each mask. This will serve as a basis for \n",
    "        stratified split between training and validation sets.\n",
    "        \n",
    "        # Returns:\n",
    "            self.train_df: training DF containing coverage information.\n",
    "        \"\"\"\n",
    "\n",
    "        print('Compute mask coverage for each observation.')\n",
    "\n",
    "        def cov_to_class(val):\n",
    "            for i in range(0, 11):\n",
    "                if val * 10 <= i:\n",
    "                    return i\n",
    "\n",
    "        # Output percentage of area covered by class\n",
    "        self.train_df['coverage'] = np.mean(self.y_train / 255., axis=(1, 2))\n",
    "        # Coverage must be split into bins, otherwise stratified split will not be possible,\n",
    "        # because each coverage will occur only once.\n",
    "        self.train_df['coverage_class'] = self.train_df.coverage.map(\n",
    "            cov_to_class)\n",
    "\n",
    "        return self.train_df\n",
    "\n",
    "    def predictions_rle_encode(self,\n",
    "                               y_pred_test,\n",
    "                               confidence_threshold_best):\n",
    "        \n",
    "        \"\"\"\n",
    "        Run Length Encoding of predictions.\n",
    "        This is needed for submission output.\n",
    "        \n",
    "        # Arguments:\n",
    "            y_pred_test: model predictions\n",
    "            confidence_threshold_best: confidence threshold, according to which\n",
    "                masks are set to 1/0.\n",
    "        # Returns:\n",
    "            y_test_pred_rle: RLEncoded predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        # If images were padded, this padding must now be removed.\n",
    "        # Otherwise encoding method will fail to properly encode predictions and\n",
    "        # score will be bad.\n",
    "        if self.pad_images:\n",
    "            print('Remove padding from images.')\n",
    "            y_min_pad, y_max_pad, x_min_pad, x_max_pad = self.padding_pixels[\n",
    "                0], self.padding_pixels[1], self.padding_pixels[2], self.padding_pixels[3]\n",
    "            y_pred_test = y_pred_test[:, y_min_pad:-\n",
    "                                      y_max_pad, x_min_pad:-x_max_pad, 0]\n",
    "            \n",
    "        # Situation is similar for previously resized images.\n",
    "        # They must be resized again to their original size before encoding.\n",
    "        else:\n",
    "            y_pred_test = np.asarray([cv2.resize(x, self.orig_image_size)\n",
    "                                      for x in y_pred_test])\n",
    "\n",
    "        assert y_pred_test.shape == (18000, 101, 101), '\\\n",
    "        Test predictions shape must be equal to (18000, 101, 101).'\n",
    "\n",
    "        print('Test predictions shape: {}'.format(y_pred_test.shape))\n",
    "\n",
    "        # Perform mask predictions binarization and RLEncoding. \n",
    "        y_test_pred_rle = {idx:\n",
    "                           rle_encode(y_pred_test[i] > confidence_threshold_best)\n",
    "                           for i, idx in enumerate(\n",
    "                               tqdm(self.test_df.index.values))}\n",
    "\n",
    "        return y_test_pred_rle\n",
    "\n",
    "    def generate_submission(self, y_test_pred_rle):\n",
    "        \n",
    "        \"\"\"\n",
    "        Submission generation based on encoded model predictions.\n",
    "        \n",
    "        # Arguments:\n",
    "            y_test_pred_rle: RLEncoded predictions.\n",
    "        # Returns:\n",
    "            submission: generated submission.\n",
    "        \"\"\"\n",
    "\n",
    "        submission = pd.DataFrame.from_dict(y_test_pred_rle, orient='index')\n",
    "        submission.index.names = ['id']\n",
    "        submission.columns = ['rle_mask']\n",
    "\n",
    "        return submission\n",
    "\n",
    "    def return_padding_borders(self):\n",
    "        \"\"\"\n",
    "        Return padding borders in case intermediate operations on original images\n",
    "        are needed.\n",
    "        \n",
    "        # Returns:\n",
    "            self.padding_pixels: tuple of padding borders.\n",
    "        \"\"\"\n",
    "        return self.padding_pixels\n",
    "\n",
    "    def __pad_image(self, img):\n",
    "        \n",
    "        \"\"\"\n",
    "        Helper function for images padding.\n",
    "        \n",
    "        # Arguments:\n",
    "            img: image as np.array\n",
    "            \n",
    "        # Returns:\n",
    "            img: padded image as np.array\n",
    "        \"\"\"\n",
    "\n",
    "        pad_floor = np.floor(\n",
    "            (np.asarray(self.image_size) - np.asarray(self.orig_image_size)) / 2)\n",
    "        pad_ceil = np.ceil((np.asarray(self.image_size) -\n",
    "                            np.asarray(self.orig_image_size)) / 2)\n",
    "\n",
    "        self.padding_pixels = np.asarray(\n",
    "            (pad_floor[0], pad_ceil[0], pad_floor[1], pad_ceil[1])).astype(np.int32)\n",
    "\n",
    "        y_min_pad, y_max_pad, x_min_pad, x_max_pad = self.padding_pixels[\n",
    "            0], self.padding_pixels[1], self.padding_pixels[2], self.padding_pixels[3]\n",
    "\n",
    "        img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad,\n",
    "                                 x_min_pad, x_max_pad,\n",
    "                                 cv2.BORDER_REFLECT_101)\n",
    "\n",
    "        assert img.shape[:2] == self.image_size, '\\\n",
    "        Image after padding must have the same shape as input image.'\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20f93da27bd8e6922bfad852edfc769c2095b433"
   },
   "source": [
    "### Define helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a68c571ea8d6f5b5b6201e987c1deada9714a503"
   },
   "outputs": [],
   "source": [
    "# Quick RLEncoding needed for submission generation.\n",
    "# Source: another kernel, thanks!\n",
    "def rle_encode(im):\n",
    "    pixels = im.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5768297e4d981d42207053fa8a138393fb5ff76"
   },
   "source": [
    "## 1. Initialize parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd70c3f34fbf6964219660e84a08e3c99ad61104"
   },
   "outputs": [],
   "source": [
    "# Input dictionary for SaltParser\n",
    "salt_parameters = {\n",
    "    'data_src': '../input/',\n",
    "    'image_size': (128, 128),\n",
    "    'pad_images': False,\n",
    "    'grayscale': False,\n",
    "}\n",
    "\n",
    "salt_parser = SaltParser(**salt_parameters)\n",
    "\n",
    "normalize = True\n",
    "save = False\n",
    "\n",
    "\n",
    "# Automatic input_dim parameter specification\n",
    "# for model training.\n",
    "input_dim = salt_parameters['image_size']\n",
    "\n",
    "if salt_parameters['grayscale']:\n",
    "    input_dim = input_dim + (1,)\n",
    "else:\n",
    "    input_dim = input_dim + (3,)\n",
    "    \n",
    "# Run name\n",
    "run_name = '{}_grayscale{}_pad{}_size{}'.format(\n",
    "    'Unet',\n",
    "    int(salt_parameters['grayscale']),\n",
    "    int(salt_parameters['pad_images']),\n",
    "    input_dim[0])\n",
    "\n",
    "print('Run name: {}'.format(run_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46665eec45583df16e464dddffcea2ff805d1550"
   },
   "source": [
    "## 2. Initialize and load data - call SaltParser functions:\n",
    "   \n",
    "1. Initialize data.\n",
    "2. Load train and test set.\n",
    "3. Compute coverage for stratified split.\n",
    "4. Return padding pixels.\n",
    "\n",
    "2.データの初期化とロード-SaltParser関数の呼び出し：\n",
    "データを初期化します。\n",
    "ロードトレインとテストセット。\n",
    "成層分割のカバレッジを計算します。\n",
    "パディングピクセルを返します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5ed4567963ac3b9eeb77ed3329d310517ecb6d0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salt_parser.initialize_data()\n",
    "X_train, y_train, X_test = salt_parser.load_data()\n",
    "train_df = salt_parser.compute_coverage()\n",
    "padding_pixels = salt_parser.return_padding_borders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d6d6bebf40a69a4d03f0c94b62f8bf541e548aa"
   },
   "source": [
    "## 3. Normalize input data to 0-1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bcb17bbea834396ba526a95ca5d8357cf9d34f5"
   },
   "outputs": [],
   "source": [
    "if normalize:\n",
    "    # X_train, X_test = utils.normalize_along_channel(X_train, X_test)\n",
    "    X_train = X_train / 255.\n",
    "    y_train = y_train / 255.\n",
    "    X_test = X_test / 255.\n",
    "    print('X_train - min: {}, max: {}'.format(np.min(X_train), np.max(X_train)))\n",
    "    print('y_train - min: {}, max: {}'.format(np.min(y_train), np.max(y_train)))\n",
    "    print('Train set: {}, {}'.format(X_train.shape, y_train.shape))\n",
    "    print('X_test - min: {}, max: {}'.format(np.min(X_test), np.max(X_test)))\n",
    "    print('Test set: {}'.format(X_test.shape))\n",
    "    \n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d49b31618e2dabede490165fd39b559b9f343e8"
   },
   "source": [
    "## 4. Perform stratified training/validation split based on coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f06ef1c577c2341bacaa901ae4fda091a22ef5b"
   },
   "outputs": [],
   "source": [
    "# Perform 80/20 training/validation split based on stratified coverage.\n",
    "X_tr, X_val, y_tr, y_val, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.2, stratify=train_df.coverage_class, random_state=1234)\n",
    "\n",
    "\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "del X_train, y_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94214b584b843bc238a60c8a9880ed705c87a1f8"
   },
   "source": [
    "## 5. Define UNet model for training.\n",
    "\n",
    "Taken from another kernel, thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3768cd5a9c9a4b90908aae3496b983bcb9b7b17d"
   },
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import (Activation, BatchNormalization, Concatenate, Conv2D,\n",
    "                          Conv2DTranspose, Dropout, Input, MaxPooling2D,\n",
    "                          UpSampling2D, concatenate)\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def conv_block(m, dim, acti, bn, res, do=0):\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(m)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    n = Dropout(do)(n) if do else n\n",
    "    n = Conv2D(dim, 3, activation=acti, padding='same')(n)\n",
    "    n = BatchNormalization()(n) if bn else n\n",
    "    return Concatenate()([m, n]) if res else n\n",
    "\n",
    "\n",
    "def level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n",
    "    if depth > 0:\n",
    "        n = conv_block(m, dim, acti, bn, res)\n",
    "        m = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n",
    "        m = level_block(m, int(inc * dim), depth - 1,\n",
    "                        inc, acti, do, bn, mp, up, res)\n",
    "        if up:\n",
    "            m = UpSampling2D()(m)\n",
    "            m = Conv2D(dim, 2, activation=acti, padding='same')(m)\n",
    "        else:\n",
    "            m = Conv2DTranspose(dim, 3, strides=2,\n",
    "                                activation=acti, padding='same')(m)\n",
    "        n = Concatenate()([n, m])\n",
    "        m = conv_block(n, dim, acti, bn, res)\n",
    "    else:\n",
    "        m = conv_block(m, dim, acti, bn, res, do)\n",
    "    return m\n",
    "\n",
    "\n",
    "def UNet(params):\n",
    "\n",
    "    img_shape = params['input_dim']\n",
    "    out_ch = 1\n",
    "    start_ch = 8\n",
    "    depth = 3\n",
    "    inc_rate = 2.\n",
    "    activation = 'relu'\n",
    "    dropout = 0.5\n",
    "    batchnorm = False\n",
    "    maxpool = True\n",
    "    upconv = True\n",
    "    residual = False\n",
    "\n",
    "    i = Input(shape=img_shape)\n",
    "    o = level_block(i, start_ch, depth, inc_rate, activation,\n",
    "                    dropout, batchnorm, maxpool, upconv, residual)\n",
    "    o = Conv2D(out_ch, 1)(o)\n",
    "    # Sigmoid activation is used because model is trained with binary_crossentropy.\n",
    "    o =  Activation('sigmoid')(o)\n",
    "\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f87f8bef5fc8d7c4ca0034f7694295d2363ddca4"
   },
   "source": [
    "## 6. Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "448382230fab79dd3fa8ab652b3e8d1581c1531f"
   },
   "outputs": [],
   "source": [
    "model = UNet({'input_dim': input_dim})\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss' ,patience=12, verbose=1, mode='min')\n",
    "model_checkpoint = ModelCheckpoint(\"./{}.h5\".format(run_name),monitor='val_loss',\n",
    "                                   save_best_only=True, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',factor=0.33, patience=6, min_lr=1e-6, verbose=1, mode='min')\n",
    "\n",
    "epochs = 10  # change to more for better score!\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "history = model.fit(X_tr, y_tr,\n",
    "                    validation_data=[X_val, y_val], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c42374acdacb8517a7f52ed8b0969746fcb51b9"
   },
   "source": [
    "## 7. Predict validation and test set masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afff0d94ef4a7aedb758f2438fe75856afdc834d"
   },
   "outputs": [],
   "source": [
    "y_pred_valid = model.predict(X_val)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "del X_tr, X_val, X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d81acb8474518e5079b1803ff6a901533a679d6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assume 0.5 threshold for mask binarization.\n",
    "# This can be optimized!\n",
    "y_pred_test_rle = salt_parser.predictions_rle_encode(\n",
    "    y_pred_test, confidence_threshold_best=0.5)\n",
    "\n",
    "submission = salt_parser.generate_submission(y_pred_test_rle)\n",
    "\n",
    "# Save submission with specified run_name.\n",
    "if save:\n",
    "    submission.to_csv('submission_{}.csv'.format(run_name))\n",
    "    \n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1a6411767b9ddcdb111fa9d3402e7a75dd67f55"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
