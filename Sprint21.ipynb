{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowsでうまくできないため、GitBashで実施\n",
    "IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大規模な映画レビューデータセットv1.0\n",
    "\n",
    "概要\n",
    "\n",
    "このデータセットには、映画レビューと関連するバイナリが含まれています\n",
    "センチメント極性ラベル。のベンチマークとして機能することを目的としています\n",
    "センチメント分類。このドキュメントでは、データセットがどのようになったかを概説します\n",
    "収集され、提供されたファイルの使用方法。\n",
    "\n",
    "データセット\n",
    "\n",
    "コアデータセットには、2万5千のトレインに均等に分割された50,000件のレビューが含まれています\n",
    "および25kテストセット。ラベルの全体的な分布はバランスが取れています（25k\n",
    "posおよび25k neg）。また、追加の50,000ラベルなし\n",
    "教師なし学習用のドキュメント。\n",
    "\n",
    "コレクション全体では、30件を超えるレビューは許可されません\n",
    "同じ映画のレビューには相関がある傾向があるため\n",
    "評価。さらに、トレインとテストセットには、互いに素なセットが含まれています。\n",
    "映画なので、暗記しても重要なパフォーマンスは得られません\n",
    "映画固有の用語と、観察されるラベルに関連付けられた用語。の中に\n",
    "ラベル付きのトレイン/テストセット、ネガティブレビューのスコアは10点中4点以下\n",
    "肯定的なレビューのスコアは10点中7点以上です。\n",
    "より中立的な評価は、トレイン/テストセットには含まれません。の中に\n",
    "教師なしセット、あらゆる評価のレビューが含まれており、\n",
    "偶数のレビュー> 5および<= 5。\n",
    "\n",
    "ファイル\n",
    "\n",
    "に対応する2つの最上位ディレクトリ[train /、test /]があります。\n",
    "トレーニングおよびテストセット。それぞれには、[pos /、neg /]ディレクトリが含まれます。\n",
    "バイナリラベルが正と負のレビュー。これらの中で\n",
    "ディレクトリ、レビューは次の名前のテキストファイルに保存されます\n",
    "規則[[id] _ [rating] .txt]ここで、[id]は一意のIDであり、[rating]は\n",
    "そのレビューの星評価を1〜10のスケールで。たとえば、ファイル\n",
    "[test / pos / 200_8.txt]は、ポジティブラベルのテストセットのテキストです。\n",
    "IMDbの一意のID 200と星評価8/10の例。の\n",
    "[train / unsup /]ディレクトリにはすべての評価に対して0があります。評価は\n",
    "データセットのこの部分では省略されます。\n",
    "\n",
    "また、レビューごとにIMDb URLを個別に含めます\n",
    "[urls_ [pos、neg、unsup] .txt]ファイル。一意のID 200のレビューは\n",
    "このファイルの200行目にURLがあります。常に変化するIMDbにより、\n",
    "レビューに直接リンクすることはできませんが、映画の\n",
    "レビューページ。\n",
    "\n",
    "レビューテキストファイルに加えて、既にトークン化されたバッグが含まれています\n",
    "私たちの実験で使用された単語（BoW）機能。これら\n",
    "train / testディレクトリの.featファイルに保存されます。各.feat\n",
    "ファイルはLIBSVM形式、ラベル付きのASCIIスパースベクトル形式\n",
    "データ。これらのファイルの機能インデックスは0から始まり、テキスト\n",
    "機能インデックスに対応するトークンは[imdb.vocab]にあります。だから\n",
    ".featファイルの0：7の行は、[imdb.vocab]の最初の単語を意味します\n",
    "（the）はそのレビューに7回登場します。\n",
    "\n",
    ".featファイル形式の詳細については、LIBSVMページ：\n",
    "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "\n",
    "また、[imdbEr.txt]には、\n",
    "（Potts、2011）によって計算された[imdb.vocab]の各トークン。期待される\n",
    "評価は、単語の平均的な極性を理解するための良い方法です\n",
    "データセット内。\n",
    "\n",
    "データセットの引用\n",
    "\n",
    "このデータセットを使用する場合、ACL 2011の論文を引用してください。\n",
    "紹介します。このペーパーには、分類結果も含まれています。\n",
    "あなたは比較したいかもしれません。\n",
    "\n",
    "\n",
    "@InProceedings {maas-EtAl：2011：ACL-HLT2011、\n",
    "  著者= {マース、アンドリューL.とデイリー、レイモンドE.とファム、ピーターT.とファン、ダンとNg、アンドリューY.とポッツ、クリストファー}、\n",
    "  title = {感情分析のための学習単語ベクトル}、\n",
    "  booktitle = {第49回計算言語学会の年次総会の議事録：Human Language Technologies}、\n",
    "  月= {6月}、\n",
    "  年= {2011}、\n",
    "  住所= {米国オレゴン州ポートランド}、\n",
    "  出版社= {計算言語学協会}、\n",
    "  ページ= {142--150}、\n",
    "  url = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "参照資料\n",
    "\n",
    "ポッツ、クリストファー。 2011.否定の否定性について。ナン・リーと\n",
    "デイビッド・ルッツ編、意味論と言語理論の議事録20\n",
    "636-659。\n",
    "\n",
    "接触\n",
    "\n",
    "質問/コメント/修正については、アンドリュー・マースまでご連絡ください\n",
    "amaas@cs.stanford.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : Everyone plays their part pretty well in this \"little nice movie\". Belushi gets the chance to live part of his life differently, but ends up realizing that what he had was going to be just as good or maybe even better. The movie shows us that we ought to take advantage of the opportunities we have, not the ones we do not or cannot have. If U can get this movie on video for around $10, it´d be an investment!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test : Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax & Director Lisa Krueger!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'y_test : (25000,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"x_train : {}\".format(x_train[2]))\n",
    "display(len(x_train))\n",
    "display(y_train.shape)\n",
    "print(\"x_test : {}\".format(x_test[0]))\n",
    "display(\"y_test : {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】BoWのスクラッチ実装 \n",
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "This movie is SOOOO funny!!!\n",
    "What a movie! I never\n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is SOOOO funny!!!\",\n",
    "    \"What a movie! I never\",\n",
    "    \"best movie ever!!!!! this movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>soooo</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  best  ever  funny  i  is  movie  never  soooo  this  what\n",
       "0  0     0     0      1  0   1      1      0      1     1     0\n",
       "1  1     0     0      0  1   0      1      1      0     0     1\n",
       "2  0     1     1      0  0   0      2      0      0     1     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SrcBoW:\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        \n",
    "        if self.token==1:\n",
    "            X_list=[]\n",
    "            for x in mini_dataset:\n",
    "                X_list+=x.split(\" \")\n",
    "            X_list=set(X_list)\n",
    "            \n",
    "            X_last_list=[[0] * len(X_list)] * len(mini_dataset)\n",
    "            for i,x1 in enumerate(mini_dataset):\n",
    "                for x2 in x1.split(\" \"):\n",
    "                    for j,x3 in enumerate(X_list):\n",
    "                        if x2==x3:\n",
    "                            X_last_list[i][j]+=1\n",
    "        if self.token==2:\n",
    "            X_list=[]\n",
    "            for x in mini_dataset:\n",
    "                X_list+=x.split(\" \")\n",
    "            X_list=set(X_list)\n",
    "            \n",
    "            X_last_list=[[0] * len(X_list)] * len(mini_dataset)\n",
    "            for i,x1 in enumerate(mini_dataset):\n",
    "                for x2 in x1.split(\" \"):\n",
    "                    for j,x3 in enumerate(X_list):\n",
    "                        if x2==x3:\n",
    "                            X_last_list[i][j]+=1\n",
    "        return X_last_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_bow=SrcBoW()\n",
    "a=src_bow.fit_transform(mini_dataset)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I',\n",
       " 'SOOOO',\n",
       " 'This',\n",
       " 'What',\n",
       " 'a',\n",
       " 'best',\n",
       " 'ever!!!!!',\n",
       " 'funny!!!',\n",
       " 'is',\n",
       " 'movie',\n",
       " 'movie!',\n",
       " 'never',\n",
       " 'this'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_list=[]\n",
    "for x in mini_dataset:\n",
    "    X_list+=x.split(\" \")\n",
    "X_list=set(X_list)\n",
    "display(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_last_list=[[0] * len(X_list)] * len(mini_dataset)\n",
    "for i,x1 in enumerate(mini_dataset):\n",
    "    for x2 in x1.split(\" \"):\n",
    "        for j,x3 in enumerate(X_list):\n",
    "            if x2==x3:\n",
    "                X_last_list[i][j]+=1\n",
    "            \n",
    "display(X_last_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-ddd60d950cf9>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-44-ddd60d950cf9>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    x = re.sub(^\"a-zA-Z0-9\", \"\", x)\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "X_list2=[]\n",
    "X_list3=[]\n",
    "X_list=[]\n",
    "for i,x in enumerate(mini_dataset):\n",
    "    x = re.sub(\"a-zA-Z0-9\", \"\", x)\n",
    "    n=0\n",
    "    l=len(x.split(\" \"))\n",
    "    for x2 in x.split(\" \"):\n",
    "        n+=1\n",
    "        l-=1\n",
    "        if n==1:\n",
    "            X_list2=x2.lower() \n",
    "        if n==2:\n",
    "            X_list2=X_list2+ \" \"+x2.lower()             \n",
    "            X_list.append(X_list2)\n",
    "            n=0\n",
    "        if l==0:\n",
    "            X_list.append(x2.lower() )\n",
    "\n",
    "display(X_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0, 0, 0], [0, 0, 0]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_last_list=[[0] * len(X_list)] * len(mini_dataset)\n",
    "for i,x1 in enumerate(mini_dataset):\n",
    "    #x1 = re.sub(r\"\\a-zA-Z0-9\", \"\", x1)\n",
    "    for x2 in x1.split(\" \"):\n",
    "        for j,x3 in enumerate(X_list):\n",
    "            if x2==x3:\n",
    "                X_last_list[i][j]+=1\n",
    "            \n",
    "display(X_last_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-4971d90c4596>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmini_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\W\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmini_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "mini_dataset = re.sub(r\"\\W\", \"\", mini_dataset)\n",
    "mini_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】TF-IDFの計算 \n",
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
    "sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation\n",
    "なお、scikit-learnでは標準的な式とは異なる式が採用されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  best  ever  movie  this\n",
       "0  0     0     0      1     1\n",
       "1  1     0     0      1     0\n",
       "2  0     1     1      2     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2014662 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "      <th>zizek</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   00  000    1        10  100   11   12   13  13th  ...   york  \\\n",
       "0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.124442  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.085006  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "5  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "6  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "7  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "8  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "9  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0   0.0  ...    0.0   \n",
       "\n",
       "      young  younger  youth    z      zero  zizek  zombie  zombies  zone  \n",
       "0  0.164505      0.0    0.0  0.0  0.136932    0.0     0.0      0.0   0.0  \n",
       "1  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "2  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "3  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "4  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "5  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "6  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "7  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "8  0.035996      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "9  0.000000      0.0    0.0  0.0  0.000000    0.0     0.0      0.0   0.0  \n",
       "\n",
       "[10 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern=r'\\b\\w+\\b', stop_words=stop_words, max_features = 5000)\n",
    "x = tfidf.fit_transform(x_train)\n",
    "#display(x)\n",
    "df_X_train = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n",
    "display(df_X_train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】TF-IDFを用いた学習 \n",
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。\n",
    "モデルは2値分類が行える任意のものを利用してください。\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "      <th>zizek</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055620</td>\n",
       "      <td>0.070033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   00  000         1        10       100   11   12   13      13th  ...   \\\n",
       "0  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  ...    \n",
       "1  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.066126  ...    \n",
       "2  0.0  0.0  0.0  0.000000  0.032718  0.000000  0.0  0.0  0.0  0.066381  ...    \n",
       "3  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  ...    \n",
       "4  0.0  0.0  0.0  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.000000  ...    \n",
       "5  0.0  0.0  0.0  0.065152  0.000000  0.000000  0.0  0.0  0.0  0.000000  ...    \n",
       "6  0.0  0.0  0.0  0.106678  0.000000  0.000000  0.0  0.0  0.0  0.089385  ...    \n",
       "7  0.0  0.0  0.0  0.000000  0.029369  0.000000  0.0  0.0  0.0  0.059586  ...    \n",
       "8  0.0  0.0  0.0  0.000000  0.000000  0.071021  0.0  0.0  0.0  0.000000  ...    \n",
       "9  0.0  0.0  0.0  0.000000  0.055620  0.070033  0.0  0.0  0.0  0.056423  ...    \n",
       "\n",
       "   york  young  younger  youth    z  zero  zizek  zombie  zombies  zone  \n",
       "0   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "1   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "2   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "3   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "4   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "5   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "6   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "7   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "8   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "9   0.0    0.0      0.0    0.0  0.0   0.0    0.0     0.0      0.0   0.0  \n",
       "\n",
       "[10 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = tfidf.vocabulary_\n",
    "#display(list(vocab.keys()))\n",
    "tfidf2 = TfidfVectorizer(vocabulary=list(vocab.keys()))\n",
    "X_test = tfidf2.fit_transform(x_test)\n",
    "df_X_test = pd.DataFrame(X_test.toarray(), columns=tfidf.get_feature_names())\n",
    "display(df_X_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0927 16:24:30.891554  1156 deprecation.py:506] From C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0927 16:24:31.079048  1156 deprecation.py:323] From C:\\Users\\anai\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5000)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                250050    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 255,251\n",
      "Trainable params: 255,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 - 6s - loss: 0.3311 - acc: 0.8600 - val_loss: 0.2932 - val_acc: 0.8768\n",
      "Epoch 2/10\n",
      "20000/20000 - 5s - loss: 0.2029 - acc: 0.9169 - val_loss: 0.3042 - val_acc: 0.8740\n",
      "Epoch 3/10\n",
      "20000/20000 - 5s - loss: 0.1067 - acc: 0.9584 - val_loss: 0.4504 - val_acc: 0.8742\n",
      "Epoch 4/10\n",
      "20000/20000 - 5s - loss: 0.0478 - acc: 0.9797 - val_loss: 0.6627 - val_acc: 0.8700\n",
      "Epoch 5/10\n",
      "20000/20000 - 5s - loss: 0.0295 - acc: 0.9895 - val_loss: 0.8152 - val_acc: 0.8560\n",
      "Epoch 6/10\n",
      "20000/20000 - 5s - loss: 0.0200 - acc: 0.9929 - val_loss: 0.8663 - val_acc: 0.8680\n",
      "Epoch 7/10\n",
      "20000/20000 - 5s - loss: 0.0128 - acc: 0.9954 - val_loss: 1.1534 - val_acc: 0.8614\n",
      "Epoch 8/10\n",
      "20000/20000 - 5s - loss: 0.0142 - acc: 0.9956 - val_loss: 1.0175 - val_acc: 0.8658\n",
      "Epoch 9/10\n",
      "20000/20000 - 5s - loss: 0.0079 - acc: 0.9970 - val_loss: 1.6636 - val_acc: 0.8676\n",
      "Epoch 10/10\n",
      "20000/20000 - 5s - loss: 0.0068 - acc: 0.9974 - val_loss: 1.5324 - val_acc: 0.8642\n",
      "y_pred [0 1 0 ... 0 1 1]\n",
      "Test loss: 22.264268334350586\n",
      "Test accuracy: 0.47856\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "th.kerasで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "# データセットの読み込み\n",
    "X_train = np.array(df_X_train)\n",
    "X_test = np.array(df_X_test)\n",
    "\n",
    "# ラベルを数値に変換\n",
    "#y[y=='Iris-versicolor'] = 0\n",
    "#y[y=='Iris-virginica'] = 1\n",
    "#y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "input_data = tf.keras.layers.Input(shape=(n_input,))\n",
    "x = tf.keras.layers.Dense(n_hidden1, activation=tf.nn.relu)(input_data)\n",
    "x = tf.keras.layers.Dense(n_hidden2, activation=tf.nn.relu)(x)\n",
    "output = tf.keras.layers.Dense(n_classes, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_data, outputs=output)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=10,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "y_pred_proba = model.predict(X_test)[:, 0]\n",
    "\n",
    "# 確率を0, 1に変換\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "\n",
    "#print(\"y_pred_proba\", y_pred_proba)\n",
    "print(\"y_pred\", y_pred)\n",
    "#結果がいらず、評価のみ行う場合はevaluateメソッドも便利です。\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】TF-IDFのスクラッチ実装 \n",
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】コーパスの前処理 \n",
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_text(text):\n",
    "    replaced_text = '\\n'.join(s.strip() for s in text.splitlines()[2:] if s != '')  # skip header by [2:]\n",
    "    replaced_text = replaced_text.lower()\n",
    "    replaced_text = re.sub(r'[【】]', ' ', replaced_text)       # 【】の除去\n",
    "    replaced_text = re.sub(r'[（）()]', ' ', replaced_text)     # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', ' ', replaced_text)   # ［］の除去\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text)  # メンションの除去\n",
    "    replaced_text = re.sub(r'https?:\\/\\/.*?[\\r\\n ]', '', replaced_text)  # URLの除去\n",
    "    replaced_text = re.sub(r'　', ' ', replaced_text)  # 全角空白の除去\n",
    "    return replaced_text\n",
    "\n",
    "def clean_url(html_text):\n",
    "    clean_text = re.sub(r'http\\S+', '', html_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
