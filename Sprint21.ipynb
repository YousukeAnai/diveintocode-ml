{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowsでうまくできないため、GitBashで実施\n",
    "IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大規模な映画レビューデータセットv1.0\n",
    "\n",
    "概要\n",
    "\n",
    "このデータセットには、映画レビューと関連するバイナリが含まれています\n",
    "センチメント極性ラベル。のベンチマークとして機能することを目的としています\n",
    "センチメント分類。このドキュメントでは、データセットがどのようになったかを概説します\n",
    "収集され、提供されたファイルの使用方法。\n",
    "\n",
    "データセット\n",
    "\n",
    "コアデータセットには、2万5千のトレインに均等に分割された50,000件のレビューが含まれています\n",
    "および25kテストセット。ラベルの全体的な分布はバランスが取れています（25k\n",
    "posおよび25k neg）。また、追加の50,000ラベルなし\n",
    "教師なし学習用のドキュメント。\n",
    "\n",
    "コレクション全体では、30件を超えるレビューは許可されません\n",
    "同じ映画のレビューには相関がある傾向があるため\n",
    "評価。さらに、トレインとテストセットには、互いに素なセットが含まれています。\n",
    "映画なので、暗記しても重要なパフォーマンスは得られません\n",
    "映画固有の用語と、観察されるラベルに関連付けられた用語。の中に\n",
    "ラベル付きのトレイン/テストセット、ネガティブレビューのスコアは10点中4点以下\n",
    "肯定的なレビューのスコアは10点中7点以上です。\n",
    "より中立的な評価は、トレイン/テストセットには含まれません。の中に\n",
    "教師なしセット、あらゆる評価のレビューが含まれており、\n",
    "偶数のレビュー> 5および<= 5。\n",
    "\n",
    "ファイル\n",
    "\n",
    "に対応する2つの最上位ディレクトリ[train /、test /]があります。\n",
    "トレーニングおよびテストセット。それぞれには、[pos /、neg /]ディレクトリが含まれます。\n",
    "バイナリラベルが正と負のレビュー。これらの中で\n",
    "ディレクトリ、レビューは次の名前のテキストファイルに保存されます\n",
    "規則[[id] _ [rating] .txt]ここで、[id]は一意のIDであり、[rating]は\n",
    "そのレビューの星評価を1〜10のスケールで。たとえば、ファイル\n",
    "[test / pos / 200_8.txt]は、ポジティブラベルのテストセットのテキストです。\n",
    "IMDbの一意のID 200と星評価8/10の例。の\n",
    "[train / unsup /]ディレクトリにはすべての評価に対して0があります。評価は\n",
    "データセットのこの部分では省略されます。\n",
    "\n",
    "また、レビューごとにIMDb URLを個別に含めます\n",
    "[urls_ [pos、neg、unsup] .txt]ファイル。一意のID 200のレビューは\n",
    "このファイルの200行目にURLがあります。常に変化するIMDbにより、\n",
    "レビューに直接リンクすることはできませんが、映画の\n",
    "レビューページ。\n",
    "\n",
    "レビューテキストファイルに加えて、既にトークン化されたバッグが含まれています\n",
    "私たちの実験で使用された単語（BoW）機能。これら\n",
    "train / testディレクトリの.featファイルに保存されます。各.feat\n",
    "ファイルはLIBSVM形式、ラベル付きのASCIIスパースベクトル形式\n",
    "データ。これらのファイルの機能インデックスは0から始まり、テキスト\n",
    "機能インデックスに対応するトークンは[imdb.vocab]にあります。だから\n",
    ".featファイルの0：7の行は、[imdb.vocab]の最初の単語を意味します\n",
    "（the）はそのレビューに7回登場します。\n",
    "\n",
    ".featファイル形式の詳細については、LIBSVMページ：\n",
    "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
    "\n",
    "また、[imdbEr.txt]には、\n",
    "（Potts、2011）によって計算された[imdb.vocab]の各トークン。期待される\n",
    "評価は、単語の平均的な極性を理解するための良い方法です\n",
    "データセット内。\n",
    "\n",
    "データセットの引用\n",
    "\n",
    "このデータセットを使用する場合、ACL 2011の論文を引用してください。\n",
    "紹介します。このペーパーには、分類結果も含まれています。\n",
    "あなたは比較したいかもしれません。\n",
    "\n",
    "\n",
    "@InProceedings {maas-EtAl：2011：ACL-HLT2011、\n",
    "  著者= {マース、アンドリューL.とデイリー、レイモンドE.とファム、ピーターT.とファン、ダンとNg、アンドリューY.とポッツ、クリストファー}、\n",
    "  title = {感情分析のための学習単語ベクトル}、\n",
    "  booktitle = {第49回計算言語学会の年次総会の議事録：Human Language Technologies}、\n",
    "  月= {6月}、\n",
    "  年= {2011}、\n",
    "  住所= {米国オレゴン州ポートランド}、\n",
    "  出版社= {計算言語学協会}、\n",
    "  ページ= {142--150}、\n",
    "  url = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "参照資料\n",
    "\n",
    "ポッツ、クリストファー。 2011.否定の否定性について。ナン・リーと\n",
    "デイビッド・ルッツ編、意味論と言語理論の議事録20\n",
    "636-659。\n",
    "\n",
    "接触\n",
    "\n",
    "質問/コメント/修正については、アンドリュー・マースまでご連絡ください\n",
    "amaas@cs.stanford.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : Everyone plays their part pretty well in this \"little nice movie\". Belushi gets the chance to live part of his life differently, but ends up realizing that what he had was going to be just as good or maybe even better. The movie shows us that we ought to take advantage of the opportunities we have, not the ones we do not or cannot have. If U can get this movie on video for around $10, it´d be an investment!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test : Don't hate Heather Graham because she's beautiful, hate her because she's fun to watch in this movie. Like the hip clothing and funky surroundings, the actors in this flick work well together. Casey Affleck is hysterical and Heather Graham literally lights up the screen. The minor characters - Goran Visnjic {sigh} and Patricia Velazquez are as TALENTED as they are gorgeous. Congratulations Miramax & Director Lisa Krueger!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'y_test : (25000,)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"x_train : {}\".format(x_train[2]))\n",
    "display(len(x_train))\n",
    "display(y_train.shape)\n",
    "print(\"x_test : {}\".format(x_test[0]))\n",
    "display(\"y_test : {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】BoWのスクラッチ実装 \n",
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "This movie is SOOOO funny!!!\n",
    "What a movie! I never\n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"This movie is SOOOO funny!!!\", \"What a movie! I never\", \"best movie ever!!!!! this movie\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_word_sentence = []\n",
    "splited_word = []\n",
    "for d in data:\n",
    "    a = d.split()\n",
    "    data_word_sentence.append(a)\n",
    "    for i in a:\n",
    "        splited_word.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_word = list(set(splited_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'movie', 'is', 'SOOOO', 'funny!!!'],\n",
       " ['What', 'a', 'movie!', 'I', 'never'],\n",
       " ['best', 'movie', 'ever!!!!!', 'this', 'movie']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'movie!',\n",
       " 'never',\n",
       " 'this',\n",
       " 'best',\n",
       " 'What',\n",
       " 'I',\n",
       " 'is',\n",
       " 'SOOOO',\n",
       " 'movie',\n",
       " 'funny!!!',\n",
       " 'ever!!!!!',\n",
       " 'a']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splited_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict1 = {}\n",
    "for i in splited_word:\n",
    "    one_list = []\n",
    "    for j in data_word_sentence:\n",
    "        one_list.append(j.count(i))\n",
    "        result_dict1[i] = one_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': [1, 0, 0],\n",
       " 'movie!': [0, 1, 0],\n",
       " 'never': [0, 1, 0],\n",
       " 'this': [0, 0, 1],\n",
       " 'best': [0, 0, 1],\n",
       " 'What': [0, 1, 0],\n",
       " 'I': [0, 1, 0],\n",
       " 'is': [1, 0, 0],\n",
       " 'SOOOO': [1, 0, 0],\n",
       " 'movie': [1, 0, 2],\n",
       " 'funny!!!': [1, 0, 0],\n",
       " 'ever!!!!!': [0, 0, 1],\n",
       " 'a': [0, 1, 0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This</th>\n",
       "      <th>movie!</th>\n",
       "      <th>never</th>\n",
       "      <th>this</th>\n",
       "      <th>best</th>\n",
       "      <th>What</th>\n",
       "      <th>I</th>\n",
       "      <th>is</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>movie</th>\n",
       "      <th>funny!!!</th>\n",
       "      <th>ever!!!!!</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This movie is SOOOO funny!!!</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What a movie! I never</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best movie ever!!!!! this movie</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 This  movie!  never  this  best  What  I  is  \\\n",
       "This movie is SOOOO funny!!!        1       0      0     0     0     0  0   1   \n",
       "What a movie! I never               0       1      1     0     0     1  1   0   \n",
       "best movie ever!!!!! this movie     0       0      0     1     1     0  0   0   \n",
       "\n",
       "                                 SOOOO  movie  funny!!!  ever!!!!!  a  \n",
       "This movie is SOOOO funny!!!         1      1         1          0  0  \n",
       "What a movie! I never                0      0         0          0  1  \n",
       "best movie ever!!!!! this movie      0      2         0          1  0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result_dict1, index=data)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2glam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_word_2gram = []\n",
    "word_sentence_2gram = []\n",
    "for l in data_word_sentence:\n",
    "    wow = []\n",
    "    for idx in range(len(l) -2 +1):\n",
    "        ab = l[idx:idx + 2]\n",
    "        d = ab[0] +\" \" + ab[1]\n",
    "        split_word_2gram.append(d)\n",
    "        wow.append(d)\n",
    "    \n",
    "    word_sentence_2gram.append(wow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This movie', 'movie is', 'is SOOOO', 'SOOOO funny!!!'],\n",
       " ['What a', 'a movie!', 'movie! I', 'I never'],\n",
       " ['best movie', 'movie ever!!!!!', 'ever!!!!! this', 'this movie']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sentence_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie',\n",
       " 'movie is',\n",
       " 'is SOOOO',\n",
       " 'SOOOO funny!!!',\n",
       " 'What a',\n",
       " 'a movie!',\n",
       " 'movie! I',\n",
       " 'I never',\n",
       " 'best movie',\n",
       " 'movie ever!!!!!',\n",
       " 'ever!!!!! this',\n",
       " 'this movie']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_word_2gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict2 = {}\n",
    "for i in split_word_2gram:\n",
    "    one_list2 = []\n",
    "    for j in word_sentence_2gram:\n",
    "        one_list2.append(j.count(i))\n",
    "        result_dict2[i] = one_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This movie': [1, 0, 0],\n",
       " 'movie is': [1, 0, 0],\n",
       " 'is SOOOO': [1, 0, 0],\n",
       " 'SOOOO funny!!!': [1, 0, 0],\n",
       " 'What a': [0, 1, 0],\n",
       " 'a movie!': [0, 1, 0],\n",
       " 'movie! I': [0, 1, 0],\n",
       " 'I never': [0, 1, 0],\n",
       " 'best movie': [0, 0, 1],\n",
       " 'movie ever!!!!!': [0, 0, 1],\n",
       " 'ever!!!!! this': [0, 0, 1],\n",
       " 'this movie': [0, 0, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>This movie</th>\n",
       "      <th>movie is</th>\n",
       "      <th>is SOOOO</th>\n",
       "      <th>SOOOO funny!!!</th>\n",
       "      <th>What a</th>\n",
       "      <th>a movie!</th>\n",
       "      <th>movie! I</th>\n",
       "      <th>I never</th>\n",
       "      <th>best movie</th>\n",
       "      <th>movie ever!!!!!</th>\n",
       "      <th>ever!!!!! this</th>\n",
       "      <th>this movie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>This movie is SOOOO funny!!!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>What a movie! I never</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best movie ever!!!!! this movie</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 This movie  movie is  is SOOOO  \\\n",
       "This movie is SOOOO funny!!!              1         1         1   \n",
       "What a movie! I never                     0         0         0   \n",
       "best movie ever!!!!! this movie           0         0         0   \n",
       "\n",
       "                                 SOOOO funny!!!  What a  a movie!  movie! I  \\\n",
       "This movie is SOOOO funny!!!                  1       0         0         0   \n",
       "What a movie! I never                         0       1         1         1   \n",
       "best movie ever!!!!! this movie               0       0         0         0   \n",
       "\n",
       "                                 I never  best movie  movie ever!!!!!  \\\n",
       "This movie is SOOOO funny!!!           0           0                0   \n",
       "What a movie! I never                  1           0                0   \n",
       "best movie ever!!!!! this movie        0           1                1   \n",
       "\n",
       "                                 ever!!!!! this  this movie  \n",
       "This movie is SOOOO funny!!!                  0           0  \n",
       "What a movie! I never                         0           0  \n",
       "best movie ever!!!!! this movie               1           1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df2 = pd.DataFrame(result_dict2, index=data)\n",
    "result_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】TF-IDFの計算 \n",
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
    "sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation\n",
    "なお、scikit-learnでは標準的な式とは異なる式が採用されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 5000)\n",
    "\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = vectorizer.fit_transform(x_test)\n",
    "\n",
    "print(x_train_tfidf.shape)\n",
    "print(x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 5000, ngram_range=(2,2))\n",
    "\n",
    "x_train_tfidf_2gram = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf_2gram = vectorizer.fit_transform(x_test)\n",
    "\n",
    "print(x_train_tfidf_2gram.shape)\n",
    "print(x_test_tfidf_2gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 1000)\n",
      "(25000, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features = 1000, ngram_range=(2,2))\n",
    "\n",
    "x_train_tfidf_1000 = vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf_1000 = vectorizer.fit_transform(x_test)\n",
    "\n",
    "print(x_train_tfidf_1000.shape)\n",
    "print(x_test_tfidf_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】TF-IDFを用いた学習 \n",
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。\n",
    "モデルは2値分類が行える任意のものを利用してください。\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lgrg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55672\n"
     ]
    }
   ],
   "source": [
    "lgrg.fit(x_train_tfidf, y_train)\n",
    "\n",
    "y_pred = lgrg.predict(x_test_tfidf)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56448\n"
     ]
    }
   ],
   "source": [
    "lgrg.fit(x_train_tfidf_2gram, y_train)\n",
    "\n",
    "y_pred = lgrg.predict(x_test_tfidf_2gram)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.56576\n"
     ]
    }
   ],
   "source": [
    "lgrg.fit(x_train_tfidf_1000, y_train)\n",
    "\n",
    "y_pred = lgrg.predict(x_test_tfidf_1000)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】TF-IDFのスクラッチ実装 \n",
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal\n",
    "tfidf_array = np.zeros((3, 13))\n",
    "for j in range(3):\n",
    "    for i in range(13):\n",
    "        n = result_df.iat[j, i]\n",
    "        s = result_df.sum(axis=1)[j]\n",
    "        N = result_df.shape[0]\n",
    "        d = (result_df.iloc[:,i] >= 1).sum()\n",
    "        \n",
    "        tf = n / s\n",
    "        idf = np.log(N / d)\n",
    "        \n",
    "        tf_idf = tf * idf\n",
    "        tfidf_array[j ,i] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21972246, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.21972246, 0.21972246, 0.08109302,\n",
       "        0.21972246, 0.        , 0.        ],\n",
       "       [0.        , 0.21972246, 0.21972246, 0.        , 0.        ,\n",
       "        0.21972246, 0.21972246, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.21972246],\n",
       "       [0.        , 0.        , 0.        , 0.21972246, 0.21972246,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.16218604,\n",
       "        0.        , 0.21972246, 0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "tfidf_sk_arr = np.zeros((3, 13))\n",
    "for j in range(3):\n",
    "    for i in range(13):\n",
    "        n = result_df.iat[j, i]\n",
    "        s = result_df.sum(axis=1)[j]\n",
    "        N = result_df.shape[0]\n",
    "        d = (result_df.iloc[:,i] >= 1).sum()\n",
    "        \n",
    "        tf = n \n",
    "        idf = np.log((1 + N) / (1 +  d)) + 1\n",
    "        \n",
    "        tf_idf = tf * idf\n",
    "        tfidf_sk_arr[j ,i] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.69314718, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.69314718, 1.69314718, 1.28768207,\n",
       "        1.69314718, 0.        , 0.        ],\n",
       "       [0.        , 1.69314718, 1.69314718, 0.        , 0.        ,\n",
       "        1.69314718, 1.69314718, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.69314718],\n",
       "       [0.        , 0.        , 0.        , 1.69314718, 1.69314718,\n",
       "        0.        , 0.        , 0.        , 0.        , 2.57536414,\n",
       "        0.        , 1.69314718, 0.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_sk_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[ 1.1524981e-02  2.7084388e-02  2.2070020e-02  1.6686095e-02\n",
      " -3.4407750e-02 -2.8236525e-05  2.3415362e-02  3.3600561e-02\n",
      " -1.2054920e-02 -1.3261908e-02]\n",
      "movieのベクトル : \n",
      "[-0.04015991  0.02022825  0.0182989   0.01375688  0.00740749 -0.00526672\n",
      "  0.0361031   0.02409956 -0.03207843 -0.00312093]\n",
      "isのベクトル : \n",
      "[ 0.03917522  0.04211482  0.03767929 -0.04038543  0.00196545 -0.03449796\n",
      "  0.04687466  0.04485184 -0.04104432 -0.00049705]\n",
      "veryのベクトル : \n",
      "[ 0.02125311 -0.00029463  0.03866521 -0.02928989  0.04804871  0.00747588\n",
      "  0.02256865 -0.00252252 -0.03878171 -0.02170852]\n",
      "goodのベクトル : \n",
      "[ 0.00412245 -0.00546295  0.04336939  0.0437059  -0.00028617 -0.03659139\n",
      " -0.00952999  0.0104233   0.0143617   0.0369311 ]\n",
      "filmのベクトル : \n",
      "[-0.01609745  0.02100273  0.01885548  0.00570064 -0.02331446 -0.0322019\n",
      "  0.02078786  0.02106917 -0.03751916 -0.04049024]\n",
      "aのベクトル : \n",
      "[ 0.01570226  0.02379345 -0.04512323  0.03353819 -0.04723103  0.01963932\n",
      " -0.03927454 -0.03842675 -0.01317669  0.0247616 ]\n",
      "badのベクトル : \n",
      "[-0.02662301 -0.02150116 -0.03786044 -0.02944371  0.011569   -0.04609672\n",
      "  0.03094245 -0.01810758  0.03896294  0.01944962]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "\n",
    "for vocab in model.wv.vocab.keys():\n",
    "    print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocabs = model.wv.vocab.keys()\n",
    "\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】コーパスの前処理 \n",
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupus = []\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = x_train[i].lower()\n",
    "    x_train[i] = x_train[i].replace(\"!\", \"\").strip()\n",
    "    x_train[i] = x_train[i].replace(\"?\", \"\").strip()\n",
    "    x_train[i] = x_train[i].replace(\"<br />\", \"\").strip()\n",
    "    x_train[i] = re.sub(r\"https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+\", \"\", x_train[i])\n",
    "    x_train[i] = x_train[i].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vecの学習\n",
    "Word2Vecの学習を行なってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22227201, 28707255)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = x_train\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】ベクトルの可視化\n",
    "得られたベクトルをt-SNEにより可視化してください。また、いくつかの単語を選びwv.most_similarを用いて似ている単語を調べてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.9659481048583984),\n",
       " ('horrible', 0.955410361289978),\n",
       " ('rarely-before-seen', 0.9495246410369873)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beckinsale´s', 0.9744152426719666),\n",
       " ('seriously,which', 0.9675507545471191),\n",
       " ('cupertino)', 0.9437820911407471)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"like\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('infidelity.set', 0.9432936906814575),\n",
       " ('rnb', 0.9428316950798035),\n",
       " ('unrevealing.4/10', 0.9389966130256653)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"by\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
