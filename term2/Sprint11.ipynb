{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint11 深層学習スクラッチ 畳み込みニューラルネットワーク1\n",
    "#### 1.このSprintについて\n",
    "##### Sprintの目的\n",
    "スクラッチを通してCNNの基礎を理解する\n",
    "\n",
    "##### どのように学ぶか\n",
    "スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。\n",
    "\n",
    "#### 2.1次元の畳み込みニューラルネットワークスクラッチ\n",
    "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
    "\n",
    "##### 1次元畳み込み層とは\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
    "\n",
    "##### データセットの用意\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。  \n",
    "$$\\alpha_i = \\sum_{s=0}^{F-1}x_{(i+s)}W_s + b$$\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$x_{(i+s})$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。  \n",
    "$$w'_s = w_s -\\alpha\\frac{\\partial L}{\\partial w_s}$$$$b' = b - \\alpha\\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : w_s に関する損失 $L$ の勾配\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}x_{(i+s}$$$$\\frac{\\partial L}{\\partial b} =  \\sum_{i=0}^{N_{out}-1}\\frac{\\partial L}{\\partial a_i}$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$: 勾配の配列のi番目の値\n",
    "\n",
    "$N_{out} : 出力のサイズ\n",
    "\n",
    "前の層に流す誤差の数式は以下です。　　\n",
    "$$\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1}\\frac{\\partial L}{\\partial a_{j-s}}w_s$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "ただし、 $j−s&lt;0$ または$j−s&gt;N_{out}−1$ のとき$\\frac{\\partial L}{\\partial a_{j-s}}=0$ です。\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】1次元畳み込み後の出力サイズの計算¶\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。  \n",
    "$$N_{out} = \\frac{N_{in} + 2P - F}{S} + 1$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルデータ\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        #self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        #self.B = initializer.B(self.n_nodes2)\n",
    "        self.W = np.array([3,5,7]) \n",
    "        self.B = np.array([1])\n",
    "        self.P = 0\n",
    "        self.Str = 1\n",
    "        self.s = len(self.W)\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        self.dX = np.array([])\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes_bf)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes_af)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #self.X = X\n",
    "        self.X = X\n",
    "        self.Xsize = len(self.X)\n",
    "        self._output_size()\n",
    "        \n",
    "        self.a = np.append(self.a, np.array([(self.X[i:i+self.s] @ self.W.T + self.B) for i in range(self.Nout)]))\n",
    "        return self.a\n",
    "    \n",
    "    def _output_size(self):\n",
    "        self.Nout = int((len(self.X) + 2*self.P - self.s) / self.Str + 1)       \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = np.append(self.dW, np.array([(dA @ self.X[i:i+self.Nout].T) for i in range(self.s)]))\n",
    "\n",
    "        for j in range(len(self.X)):\n",
    "            if j-self.Nout < 0:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ : self.s-(self.Nout-j) ]))\n",
    "                #display(dA[:j+1])\n",
    "                self.dX = np.append(self.dX, (dA[:j+1] @ np.flip(self.W[ : self.s-(self.Nout-j) ]).T))\n",
    "                \n",
    "            elif j > len(self.X) - self.Nout:\n",
    "                #display(j)\n",
    "                #display(np.flip(dA    [-(j-(self.Xsize - self.Nout)):]))\n",
    "                #display(np.flip(self.W[-(j-(self.Xsize - self.Nout)):]))\n",
    "                self.dX = np.append(self.dX, (np.flip(dA    [-(j-(self.Xsize - self.Nout)):])) @ \n",
    "                                              np.flip(self.W[-(j-(self.Xsize - self.Nout)):]).T)\n",
    "            else:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ j-self.Nout+1 : j+1 ]))\n",
    "                self.dX = np.append(self.dX, (dA @ np.flip(self.W[ j-self.Nout+1 : j+1 ]).T))\n",
    "\n",
    "        # 更新\n",
    "        #self.W = self.optimizer.update(dW, self.W)\n",
    "        #self.B = self.optimizer.update(dB, self.B)\n",
    "\n",
    "        return self.dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "入力$x$、重み$w$、バイアス$b$を次のようにします。\n",
    "\n",
    "x = np.array([1,2,3,4])  \n",
    "w = np.array([3, 5, 7])  \n",
    "b = np.array([1])  \n",
    "フォワードプロパゲーションをすると出力は次のようになります。\n",
    "\n",
    "a = np.array([35, 50])  \n",
    "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
    "\n",
    "delta_a = np.array([10, 20])  \n",
    "バックプロパゲーションをすると次のような値になります。\n",
    "\n",
    "delta_b = np.array([30])  \n",
    "delta_w = np.array([50, 80, 110])  \n",
    "delta_x = np.array([30, 110, 170, 140])  \n",
    "##### 実装上の工夫\n",
    "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。  \n",
    "$$a_i = \\sum_{s=0}^{F-1}x_{i+s}w_s + b$$\n",
    "\n",
    "バイアス項は単純な足し算のため、重みの部分を見ます。$$\\sum_{s=0}^{F-1}x_{i+s}w_s$$\n",
    "\n",
    "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。\n",
    "\n",
    "x = np.array([1, 2, 3, 4])  \n",
    "w = np.array([3, 5, 7])\n",
    "\n",
    "a = np.empty((2, 3))  \n",
    "\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)  \n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である  \n",
    "\n",
    "a = a.sum(axis=1)  \n",
    "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
    "\n",
    "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
    "\n",
    "x = np.array([1, 2, 3, 4])  \n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "\n",
    "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])  \n",
    "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
    "\n",
    "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
    "\n",
    "##### 《参考》\n",
    "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
    "\n",
    "Indexing — NumPy v1.17 Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "test = FC(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30., 110., 170., 140.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_a = np.array([10, 20])\n",
    "test.backward(delta_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 50.,  80., 110.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test.dB)\n",
    "display(test.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.optimizer = optimizer\n",
    "        #self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        #self.B = initializer.B(self.n_nodes2)\n",
    "        self.W = np.array([3,5,7]) \n",
    "        self.B = np.array([1])\n",
    "        self.P = 0\n",
    "        self.Str = 1\n",
    "        self.s = len(self.W)\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        self.dX = np.array([])\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes_bf)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes_af)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        #self.X = X\n",
    "        self.X = X\n",
    "        self.Xsize = len(self.X)\n",
    "        self._output_size()\n",
    "        \n",
    "        self.a = np.append(self.a, np.array([(self.X[i:i+self.s] @ self.W.T + self.B) for i in range(self.Nout)]))\n",
    "        return self.a\n",
    "    \n",
    "    def _output_size(self):\n",
    "        self.Nout = int((len(self.X) + 2*self.P - self.s) / self.Str + 1)       \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = np.append(self.dW, np.array([(dA @ self.X[i:i+self.Nout].T) for i in range(self.s)]))\n",
    "\n",
    "        for j in range(len(self.X)):\n",
    "            if j-self.Nout < 0:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ : self.s-(self.Nout-j) ]))\n",
    "                #display(dA[:j+1])\n",
    "                self.dX = np.append(self.dX, (dA[:j+1] @ np.flip(self.W[ : self.s-(self.Nout-j) ]).T))\n",
    "                \n",
    "            elif j > len(self.X) - self.Nout:\n",
    "                #display(j)\n",
    "                #display(np.flip(dA    [-(j-(self.Xsize - self.Nout)):]))\n",
    "                #display(np.flip(self.W[-(j-(self.Xsize - self.Nout)):]))\n",
    "                self.dX = np.append(self.dX, (np.flip(dA    [-(j-(self.Xsize - self.Nout)):])) @ \n",
    "                                              np.flip(self.W[-(j-(self.Xsize - self.Nout)):]).T)\n",
    "            else:\n",
    "                #display(j)\n",
    "                #display(np.flip(self.W[ j-self.Nout+1 : j+1 ]))\n",
    "                self.dX = np.append(self.dX, (dA @ np.flip(self.W[ j-self.Nout+1 : j+1 ]).T))\n",
    "\n",
    "        # 更新\n",
    "        #self.W = self.optimizer.update(dW, self.W)\n",
    "        #self.B = self.optimizer.update(dB, self.B)\n",
    "\n",
    "        return self.dX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成 \n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、\n",
    "\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。  \n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。  \n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）  \n",
    "出力は次のようになります。\n",
    "\n",
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。  \n",
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "##### 《補足》\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.P = 0\n",
    "        self.Str = 1\n",
    "        self.a = np.array([])\n",
    "        self.dW = np.array([])\n",
    "        self.dX = np.array([])\n",
    "        #self.s=None\n",
    "        \n",
    "    def forward(self, X, W ,B):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes_bf)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes_af)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Xsize = self.X.shape[1]\n",
    "        self.W = W \n",
    "        self.B = B\n",
    "        self.s = self.W.shape[2]\n",
    "        \n",
    "        self._output_size()\n",
    "        self.a = np.append(self.a, np.array([np.sum(self.X[:, i:i+self.s] * self.W[i0,:,:]) + self.B[i0] \n",
    "                                             for i0,i in itertools.product(range(self.W.shape[0]), range(self.Nout))]))\n",
    "        self.a = self.a.reshape(self.W.shape[0], self.Nout)\n",
    "        return self.a\n",
    "    \n",
    "    def _output_size(self):\n",
    "        self.Nout = int((self.Xsize + 2*self.P - self.s) / self.Str + 1)       \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=1)   \n",
    "        \n",
    "        for i0 in range(self.W.shape[0]):\n",
    "            dX_ary = np.zeros((self.W.shape[1], self.W.shape[2]))\n",
    "            for i in range(self.Nout):\n",
    "                dX_ary += dA[i0,i] * self.X[:, i:i+self.s]\n",
    "            self.dW = np.append(self.dW, dX_ary)\n",
    "        self.dW = self.dW.reshape(3,2,3)\n",
    "        \"\"\"検討中\n",
    "        self.Xp=np.array([self.X]*self.W.shape[0])\n",
    "        self.\n",
    "        self.dW= np.zeros((self.W.shape))\n",
    "        self.dW = [self.dW + (dA * self.Xp[:,:,i:i+self.s]) for i in range(self.Nout)]\n",
    "        \"\"\"\n",
    "        #self.dW = np.append(self.dW, [dX_ary + (np.ones((self.W[1].shape, self.W[2].shape))*dA[i0,i]) * self.X[:, i:i+self.Nout] \n",
    "        #                              for i in range(self.Nout)] for i0 in range(self.W.shape[0]))\n",
    "        \n",
    "        #for j in range(len(self.X)):\n",
    "        #    if j-self.Nout < 0:\n",
    "        #        self.dX = np.append(self.dX, (dA[:j+1] @ np.flip(self.W[ : self.s-(self.Nout-j) ]).T))\n",
    "        #        \n",
    "        #    elif j > len(self.X) - self.Nout:\n",
    "        #        self.dX = np.append(self.dX, (np.flip(dA    [-(j-(self.Xsize - self.Nout)):])) @ \n",
    "        #                                      np.flip(self.W[-(j-(self.Xsize - self.Nout)):]).T)\n",
    "        #    else:\n",
    "        #        self.dX = np.append(self.dX, (dA @ np.flip(self.W[ j-self.Nout+1 : j+1 ]).T))\n",
    "\n",
    "        return self.dW\n",
    "    \n",
    "    #def _naiseki_plus(self,dA):\n",
    "    #    dX_ary = np.zeros((self.W[1].shape, self.W[2].shape))\n",
    "    #    dX_ary = [dX_ary + (np.ones((self.W[1].shape, self.W[2].shape))*dA[i0,i]) * self.X[:, i:i+self.Nout] for i in range(self.Nout)]\n",
    "    #    return dX_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2dim = Conv1d()\n",
    "test2dim.forward(x,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[2, 3, 4, 5], [1, 2, 3, 4]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "#w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "w = np.array([[[1,1,1],\n",
    "            [1,1,1]],\n",
    "            [[1,1,1],\n",
    "            [2,1,1]],\n",
    "            [[2,1,1],\n",
    "            [1,1,2]]])\n",
    "b = np.array([3, 2, 1]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18., 24.],\n",
       "       [18., 25.],\n",
       "       [21., 29.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2dim = Conv1d()\n",
    "test2dim.forward(x,w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バックプロパゲーションの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = np.array([[52,56],\n",
    "            [32,35],\n",
    "            [9,11]])\n",
    "#dA = np.array([9,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[52, 56],\n",
       "        [32, 35],\n",
       "        [ 9, 11]],\n",
       "\n",
       "       [[52, 56],\n",
       "        [32, 35],\n",
       "        [ 9, 11]],\n",
       "\n",
       "       [[52, 56],\n",
       "        [32, 35],\n",
       "        [ 9, 11]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA3=np.array([dA]*3)\n",
    "dA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[52, 52, 52],\n",
       "        [56, 56, 56]],\n",
       "\n",
       "       [[32, 32, 32],\n",
       "        [35, 35, 35]],\n",
       "\n",
       "       [[ 9,  9,  9],\n",
       "        [11, 11, 11]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dA3=dA3.transpose(1,2,0)\n",
    "dA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[272., 380., 488.],\n",
       "        [164., 272., 380.]],\n",
       "\n",
       "       [[169., 236., 303.],\n",
       "        [102., 169., 236.]],\n",
       "\n",
       "       [[ 51.,  71.,  91.],\n",
       "        [ 31.,  51.,  71.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2dim.backward(dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】学習と推定 \n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(X_train, t_train), (X_test, t_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train  = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "t_train_one_hot = enc.fit_transform(t_train[:, np.newaxis])\n",
    "t_test_one_hot = enc.fit_transform(t_test[:,  np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 1, 784)\n",
    "X_test = X_test.reshape(-1, 1, 784)\n",
    "w = np.ones((3, 1, 3))\n",
    "b = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_mnist_FC:\n",
    "    \"\"\"\n",
    "    畳み込み層\n",
    "    Parameters\n",
    "    ----------\n",
    "    w:畳み込み層の重み　w.shape  (出力チャネル、入力チャネル、フィルターサイズ)\n",
    "    b:畳み込み層のバイアス　b.shape (出力チャネル, )\n",
    "    stride:ストライド数\n",
    "    padding:パディング数\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, w, b , optimizer,stride, padding):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = w\n",
    "        self.B = b\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.A = X\n",
    "        output_size, chanel_size, filter_size = self.W.shape\n",
    "        feature_size = self.A.shape[2]\n",
    "        sample_size = self.A.shape[0]\n",
    "\n",
    "        a = np.zeros([sample_size, output_size, feature_size-2])\n",
    "        for samples in range(sample_size):\n",
    "            for output in range(output_size):\n",
    "                for j in range(filter_size - 1):\n",
    "                    sig = 0\n",
    "                    for chanel in range(chanel_size):\n",
    "                        for i in range(filter_size):\n",
    "                            sig += X[samples, chanel, i+j] * self.W[output, chanel, j]\n",
    "                    a[samples, output, j] = sig + b[output]\n",
    "        \n",
    "        return a\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.n_out = N_OUT(self.stride, self.padding, self.W, self.A)\n",
    "        \n",
    "        output_size, chanel_size, filter_size = self.W.shape\n",
    "        feature_size = self.A.shape[2]\n",
    "        sample_size = self.A.shape[0]\n",
    "        \n",
    "        #LBの計算\n",
    "        self.LB = dA.sum(axis=0)\n",
    "        self.LB = self.LB.sum(axis=1)\n",
    "        \n",
    "        #LWの計算\n",
    "        self.LW = np.zeros_like(self.W)\n",
    "        for samples in range(sample_size):\n",
    "            for output in range(output_size):\n",
    "                for chanel in range(chanel_size):\n",
    "                    for i in range(filter_size):\n",
    "                        for j in range(filter_size -1):\n",
    "                            self.LW[output, chanel, i] += dA[samples, output, j]*self.A[samples, chanel, j+i]\n",
    "                        \n",
    "                        \n",
    "\n",
    "                    \n",
    "                    \n",
    "        #dZの計算\n",
    "        dZ = np.zeros_like(self.A)\n",
    "        for samples in range(sample_size):\n",
    "            for output in range(output_size):\n",
    "                for chanel in range(chanel_size):\n",
    "                    for j in range(feature_size):\n",
    "                        sigma=0\n",
    "                        for s in range(filter_size):\n",
    "                            if j - s < 0 or j - s > self.n_out -1:\n",
    "                                pass\n",
    "                            else:\n",
    "                                sigma += dA[samples, output,  j-s] * self.W[output, chanel, s]\n",
    "                        dZ[samples, chanel, j] += sigma\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def forward(self, X):\n",
    "        self.A = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, Z):\n",
    "        \n",
    "        return Z * np.maximum(np.sign(self.A), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        init = initializer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.W = init.W(n_nodes1, n_nodes2)\n",
    "        self.B = init.B(n_nodes2)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.z = X\n",
    "        self.a = X@self.W + self.B\n",
    "        \n",
    "        return self.a\n",
    "\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = dA @ self.W.T\n",
    "        self.LW = self.z.T @ dA\n",
    "        self.LB = np.sum(dA, axis=0)\n",
    "        \n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B  = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, A):\n",
    "        exp_a = np.exp(A)\n",
    "        softmax_result = np.empty((A.shape[0], A.shape[1]))\n",
    "        exp_sum = np.sum(exp_a, axis=1)\n",
    "        for i in range(A.shape[0]):\n",
    "            softmax_result[i] = exp_a[i] / exp_sum[i]\n",
    "            \n",
    "        return softmax_result\n",
    "    \n",
    "    def backward(self, Z, Y):\n",
    "        \n",
    "        L_A = Z - Y\n",
    "        self.cross_entropy = -np.average(np.sum(Y*np.log(Z), axis=1))\n",
    "        \n",
    "        \n",
    "        return L_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        layer.W = layer.W -  self.lr * layer.LW\n",
    "        \n",
    "        layer.B = layer.B - self.lr*layer.LB\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_OUT(stride, padding, X,  W):\n",
    "    if X.ndim == 1:\n",
    "        return int((X.shape[0] + (2*padding) - len(W) / stride) + 1)\n",
    "    elif X.ndim == 3:\n",
    "        return int((X.shape[2] + (2*padding) - len(W) / stride) + 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_mnist = CNN_mnist_FC(w, b, SGD(0.1), 1, 0)\n",
    "A = cnn_mnist.forward(X_train)\n",
    "relu = Relu()\n",
    "A_relu = relu.forward(A)\n",
    "A_flat = A_relu.reshape(A_relu.shape[0], -1)\n",
    "FC_1 = FC(2346, 10, SimpleInitializer(0.1), SGD(0.1))\n",
    "A_FC_1 = FC_1.forward(A_flat)\n",
    "softmax = Softmax()\n",
    "A_soft = softmax.forward(A_FC_1)\n",
    "A_delta = softmax.backward(A_soft, t_train_one_hot)\n",
    "delta_Z = FC_1.backward(A_delta)\n",
    "delta_Z_reshape = delta_Z.reshape(A_relu.shape)\n",
    "delta_Z_relu = relu.backward(delta_Z_reshape)\n",
    "dZ = cnn_mnist.backward(delta_Z_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0982\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.reshape(-1, 1, 784)\n",
    "t_A = cnn_mnist.forward(X_test)\n",
    "t_A = relu.forward(t_A)\n",
    "t_A  = t_A.reshape(t_A.shape[0], -1)\n",
    "t_A = FC_1.forward(t_A)\n",
    "C = np.max(t_A, axis=1)\n",
    "for i in range(t_A.shape[0]):\n",
    "    t_A[i] = np.exp(t_A[i] - C[i])\n",
    "t_A = softmax.forward(t_A)\n",
    "y = np.argmax(t_A, axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(t_test, y))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
