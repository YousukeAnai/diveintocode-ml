{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprint22 深層学習スクラッチ リカレントニューラルネットワーク\n",
    "\n",
    "#### 1.このSprintについて\n",
    "\n",
    "##### Sprintの目的\n",
    "スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n",
    "\n",
    "##### どのように学ぶか\n",
    "スクラッチでリカレントニューラルネットワークの実装を行います。\n",
    "\n",
    "#### 2.リカレントニューラルネットワークスクラッチ\n",
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
    "\n",
    "### 【問題1】SimpleRNNのフォワードプロパゲーション実装 \n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。  \n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。  \n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$a_t = x_t \\cdot W_x + h_{t-1} \\cdot W_h + B$$$$h_t = tanh(a_t)$$\n",
    "\n",
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "$x_t$ : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "$W_x$ : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "$h_{t−1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "$W_h$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "$B$ : バイアス項 (n_nodes,)\n",
    "\n",
    "初期状態 $h_0$ は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 $x$ は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchSimpleRNNClassifier():\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bp : int\n",
    "        バックプロパゲーション回数\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x, w_x, w_h):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        self.batch_size = x.shape[0] # 1\n",
    "        self.n_sequences = x.shape[1] # 3\n",
    "        self.n_features = x.shape[2] # 2\n",
    "        self.n_nodes = w_x.shape[1] # 4\n",
    "        h = np.zeros((self.batch_size, self.n_nodes)) # (batch_size, n_nodes)\n",
    "        \n",
    "        for c in range(self.batch_size):\n",
    "            for i in range(self.n_sequences):\n",
    "                if i==0:\n",
    "                    a = np.dot(x[c,i,:],w_x) + np.dot(h, w_h) + b\n",
    "                    a=np.tanh(a)\n",
    "                else:\n",
    "                    a = np.dot(x[c,i,:],w_x) + np.dot(a, w_h) + b\n",
    "                    a=np.tanh(a)\n",
    "        return  a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】小さな配列でのフォワードプロパゲーションの実験 \n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01, 0.02],\n",
       "        [0.02, 0.03],\n",
       "        [0.03, 0.04]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(x)\n",
    "display(w_x.shape)\n",
    "display(w_h.shape)\n",
    "display(h.shape)\n",
    "display(n_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76188798, 0.76213958, 0.76239095, 0.76255841],\n",
       "       [0.76205574, 0.76247469, 0.76289301, 0.76318545],\n",
       "       [0.7622234 , 0.76280939, 0.76339414, 0.76381105]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x.reshape(x.shape[1],x.shape[2])\n",
    "a=np.dot(x[0,:],w_x) + np.dot(h, w_h) + b\n",
    "a=np.tanh(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in range(batch_size):\n",
    "    for i in range(n_sequences):\n",
    "        if i==0:\n",
    "            a = np.dot(x[c,i,:],w_x) + np.dot(h, w_h) + b\n",
    "            a=np.tanh(a)\n",
    "        else:\n",
    "            a = np.dot(x[c,i,:],w_x) + np.dot(a, w_h) + b\n",
    "            a=np.tanh(a)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_RNN=ScratchSimpleRNNClassifier()\n",
    "A=src_RNN.fit(x, w_x, w_h)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#答え\n",
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
