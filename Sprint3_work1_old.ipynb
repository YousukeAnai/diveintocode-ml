{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】仮定関数 \n",
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "      \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "    self.loss_list : list, []\n",
    "      学習用データのイテレーション毎の損失関数リスト\n",
    "    self.loss_list : list, []\n",
    "      検証用データのイテレーション毎の損失関数リスト\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.loss_list = []\n",
    "        self.loss_val_list = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.val : int64\n",
    "          検証用データの有無を識別。　0：無  1：有\n",
    "        self.Θj : 次の形のndarray, shape (n_samples, )\n",
    "          学習用データのΘ\n",
    "        self.Θj_val : 次の形のndarray, shape (n_samples, )\n",
    "          検証用データのΘ\n",
    "        \"\"\"\n",
    "        self.X_ = np.concatenate([np.ones((X.shape[0],1)), X], axis=1)\n",
    "        self.y_ = y\n",
    "        self.val = 0\n",
    "                \n",
    "        self.Θj = np.random.rand(1, self.X_.shape[1])\n",
    "\n",
    "\n",
    "        for iter_n in range(self.iter):\n",
    "            self._linear_hypothesis()\n",
    "            self._gradient_descent()\n",
    "            self._cost_function()\n",
    "            \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程を出力\n",
    "                print(\"ーー{}回目y~ーーーー\\n{}\".format(iter_n+1, self.h))\n",
    "                print(\"ーー{}回目Θーーーー\\n{}\".format(iter_n+2, self.Θj))\n",
    "        \n",
    "        if (X_val is not None and y_val is not None):\n",
    "            self.val = 1\n",
    "            self.X_val_ = np.concatenate([np.ones((X_val.shape[0],1)), X_val], axis=1)\n",
    "            self.y_val_ = y_val\n",
    "            self.Θj_val = np.random.rand(1, self.X_val_.shape[1])\n",
    "\n",
    "            for iter_n in range(self.iter):\n",
    "                self._linear_hypothesis_val()\n",
    "                self._gradient_descent_val()\n",
    "                self._cost_function_val()\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"ーー{}回目y~ーーーー\\n{}\".format(iter_n+1, self.h_val))\n",
    "                    print(\"ーー{}回目Θーーーー\\n{}\".format(iter_n+2, self.Θj_val))\n",
    "        \n",
    "    def _linear_hypothesis(self):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する    \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.h : 次の形のndarray, shape (n_samples, )\n",
    "          学習用データの仮定関数の計算結果\n",
    "        \"\"\"\n",
    "        self.h = np.dot(self.X_, self.Θj.T)\n",
    "    def _linear_hypothesis_val(self):\n",
    "        \"\"\"\n",
    "        検証データ用の_linear_hypothesis()     \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.h_val : 次の形のndarray, shape (n_samples, )\n",
    "          学習用データの仮定関数の計算結果\n",
    "        \"\"\"\n",
    "        self.h_val = np.dot(self.X_val_, self.Θj_val.T)\n",
    "            \n",
    "    def _cost_function(self):\n",
    "        \"\"\"\n",
    "        線形の目的関数により、損失を算出する。(学習データ専用)\n",
    "        Parameters\n",
    "        ----------\n",
    "        JΘ : float64\n",
    "            目的関数による、損失計算のうち、サンプル数で割る前の段階の値\n",
    "        \"\"\"\n",
    "        JΘ =  ((self.h - self.y_)**2).sum()\n",
    "        self.loss = JΘ/(self.y_.shape[0]*2)  \n",
    "        self.loss_list.append(self.loss)\n",
    "    def _cost_function_val(self):\n",
    "        \"\"\"\n",
    "        検証データ用の _cost_function()     \n",
    "        Parameters\n",
    "        ----------\n",
    "        JΘ_val : float64\n",
    "            目的関数による、損失計算のうち、サンプル数で割る前の段階の値\n",
    "        \"\"\"        \n",
    "        JΘ_val = ((self.h_val - self.y_val_)**2).sum()\n",
    "        self.loss_val = JΘ_val/(self.y_val_.shape[0]*2)        \n",
    "        self.loss_val_list.append(self.loss_val)\n",
    "\n",
    "    def _gradient_descent(self):\n",
    "        \"\"\"\n",
    "        再急降下法により、次回イテレーションのΘを算出する。(学習データ専用)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Δj : 次の形のndarray, shape (n_samples, )\n",
    "            現状のΘにおける勾配\n",
    "        Attributes\n",
    "        ----------\n",
    "        self.Θj : 次の形のndarray, shape (n_samples, )\n",
    "          次回イテレーションのΘ\n",
    "        \"\"\"\n",
    "        Δj =  np.dot((self.h - self.y_).T , self.X_)\n",
    "        self.Θj -= self.lr*Δj\n",
    "    def _gradient_descent_val(self):\n",
    "        \"\"\"\n",
    "        検証データ用の _gradient_descent()     \n",
    "        Parameters\n",
    "        ----------\n",
    "        Δj_val : 次の形のndarray, shape (n_samples, )\n",
    "            現状のΘにおける勾配\n",
    "        Attributes\n",
    "        ----------\n",
    "        self.Θj_val : 次の形のndarray, shape (n_samples, )\n",
    "          次回イテレーションのΘ\n",
    "        \"\"\"\n",
    "        Δj_val =  np.dot((self.h_val - self.y_val_).T , self.X_val_)\n",
    "        self.Θj_val -= self.lr*Δj_val\n",
    "\n",
    "    def graph_cost_func(self):\n",
    "        \"\"\"\n",
    "        損失の推移をグラフ化する。    \n",
    "        検証用データが入力されていれば、学習用と検証用の損失推移を重ねてグラフ化\n",
    "        \"\"\"\n",
    "        plt.title(\"Num_of_Iteration vs Loss\")\n",
    "        plt.xlabel(\"Num_of_Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        a=range(self.iter)\n",
    "        plt.plot(range(1,self.iter+1), self.loss_list, color=\"b\", marker=\"o\", label=\"train_loss\")\n",
    "        if self.val is 1:\n",
    "            plt.plot(range(1,self.iter+1), self.loss_val_list, color=\"g\", marker=\"+\", label=\"val_loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        X = np.concatenate([np.ones((X.shape[0],1)), X], axis=1)\n",
    "        self.h = np.dot(X, self.Θj.T)\n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】最急降下法 \n",
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fit\n",
    "メソッドから呼び出すようにしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】推定 \n",
    "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
    "仮定関数 hθ(x)の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ーー1回目y~ーーーー\n",
      "[[-0.67723574]\n",
      " [-0.23980733]\n",
      " [ 0.19762107]\n",
      " [ 0.63504947]\n",
      " [ 1.07247787]]\n",
      "ーー2回目Θーーーー\n",
      "[[13.59881053  6.56129785  6.16662421]]\n",
      "ーー2回目y~ーーーー\n",
      "[[-4.40118947]\n",
      " [ 4.59881053]\n",
      " [13.59881053]\n",
      " [22.59881053]\n",
      " [31.59881053]]\n",
      "ーー3回目Θーーーー\n",
      "[[20.29940527  6.56129785  6.16662421]]\n",
      "ーー3回目y~ーーーー\n",
      "[[ 2.29940527]\n",
      " [11.29940527]\n",
      " [20.29940527]\n",
      " [29.29940527]\n",
      " [38.29940527]]\n",
      "ーー4回目Θーーーー\n",
      "[[23.64970263  6.56129785  6.16662421]]\n",
      "ーー4回目y~ーーーー\n",
      "[[ 5.64970263]\n",
      " [14.64970263]\n",
      " [23.64970263]\n",
      " [32.64970263]\n",
      " [41.64970263]]\n",
      "ーー5回目Θーーーー\n",
      "[[25.32485132  6.56129785  6.16662421]]\n",
      "ーー5回目y~ーーーー\n",
      "[[ 7.32485132]\n",
      " [16.32485132]\n",
      " [25.32485132]\n",
      " [34.32485132]\n",
      " [43.32485132]]\n",
      "ーー6回目Θーーーー\n",
      "[[26.16242566  6.56129785  6.16662421]]\n",
      "ーー最終y~ーーーー\n",
      " [[ 8.16242566]\n",
      " [17.16242566]\n",
      " [26.16242566]\n",
      " [35.16242566]\n",
      " [44.16242566]]\n",
      "ーーXーーーー\n",
      " [[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "ーーyーーーー\n",
      " [[ 9]\n",
      " [18]\n",
      " [27]\n",
      " [36]\n",
      " [45]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anai\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scr_line_reg = ScratchLinearRegression(5, 0.1, False, True)\n",
    "X = np.arange(10).reshape(5,2)\n",
    "y = np.array([[9], [18], [27], [36], [45]])\n",
    "\n",
    "std = StandardScaler()\n",
    "X_std = std.fit_transform(X)\n",
    "\n",
    "scr_line_reg.fit(X_std, y)\n",
    "predict_y = scr_line_reg.predict(X_std)\n",
    "print(\"ーー最終y~ーーーー\\n\",format(predict_y))\n",
    "print(\"ーーXーーーー\\n\",format(X))\n",
    "print(\"ーーyーーーー\\n\",format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題4】平均二乗誤差 \n",
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
    "平均二乗誤差は以下の数式で表されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    L =  ((y_pred - y)**2).sum()    \n",
    "    mse = L/y.shape[0]\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7015307776687332"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse=MSE(predict_y, y)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】目的関数 \n",
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
    "目的関数（損失関数） \n",
    "J(θ)は次の式です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_line_reg = ScratchLinearRegression(100, 0.01, False, False)\n",
    "X = np.arange(10).reshape(5,2)\n",
    "y = np.array([[9], [18], [27], [36], [45]])\n",
    "#y = np.arange(1100).reshape(1100,1)\n",
    "std = StandardScaler()\n",
    "X_std = std.fit_transform(X)\n",
    "\n",
    "scr_line_reg.fit(X_std, y, X_std, y)\n",
    "scr_line_reg._cost_function()\n",
    "scr_line_reg._cost_function_val()\n",
    "\n",
    "display(scr_line_reg.loss)\n",
    "display(scr_line_reg.loss_val)\n",
    "\n",
    "predict_y = scr_line_reg.predict(X_std)\n",
    "print(\"ーー最終y~ーーーー\\n\",format(predict_y))\n",
    "#print(\"ーーXーーーー\\n\",format(X))\n",
    "#print(\"ーーyーーーー\\n\",format(y))\n",
    "print(\"SK-Learn_LineReg  MY_MSE：{}\".format(MSE(predict_y, y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】学習と推定 \n",
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "X = df.drop(\"SalePrice\",axis=1)\n",
    "y = df.loc[:,[\"SalePrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option(\"display.max_colwidth\", 100)\n",
    "#pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 1000)\n",
    "y.describe()\n",
    "display(X.select_dtypes([\"int64\",\"float64\"]).isnull().sum())\n",
    "X['LotFrontage'].plot.hist(title = 'LotFrontage Histogram')\n",
    "plt.xlabel('LotFrontage')\n",
    "plt.show()\n",
    "X['MasVnrArea'].plot.hist(title = 'MasVnrArea Histogram')\n",
    "plt.xlabel('MasVnrArea')\n",
    "plt.show()\n",
    "X['GarageYrBlt'].plot.hist(title = 'GarageYrBlt Histogram')\n",
    "plt.xlabel('GarageYrBlt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"LotFrontage\"]=X[\"LotFrontage\"].fillna(X[\"LotFrontage\"].mode()[0])\n",
    "X[\"MasVnrArea\"]=X[\"MasVnrArea\"].fillna(X[\"MasVnrArea\"].mode()[0])\n",
    "X[\"GarageYrBlt\"]=X[\"GarageYrBlt\"].fillna(X[\"GarageYrBlt\"].mode()[0])\n",
    "X.select_dtypes([\"int64\",\"float64\"]).isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select_dtypes(\"object\").isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.drop([\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select_dtypes(\"object\").apply(pd.Series.nunique, axis=0)\n",
    "#X.select_dtypes('object')[X.select_dtypes('object').isnull().sum()>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value_num in X.select_dtypes('object').columns:\n",
    "    display(df[value_num].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X.select_dtypes(\"object\").isnull().any().index[X.select_dtypes(\"object\").isnull().sum()!=0]:\n",
    "    X[column]=X[column].fillna(X[column].mode()[0])\n",
    "    \n",
    "X.select_dtypes(\"object\").isnull().any()\n",
    "X.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1HOT\n",
    "X = pd.get_dummies(X, columns=X.select_dtypes(\"object\").columns)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "X = df.loc[:,[\"GrLivArea\", \"YearBuilt\"]]\n",
    "y = df.loc[:,[\"SalePrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)\n",
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_reg = LinearRegression().fit(X_train_std, y_train) \n",
    "\n",
    "scr_line_reg = ScratchLinearRegression(200, 0.0001, False, False)\n",
    "scr_line_reg.fit(X_train_std, y_train)\n",
    "predict_y = scr_line_reg.predict(X_test_std)\n",
    "\n",
    "print(\"SK-Learn_LineReg  MSE：{}\".format(mean_squared_error(y_test, line_reg.predict(X_test_std))))\n",
    "print(\"Scratch_LineReg  MSE：{}\".format(mean_squared_error(y_test, predict_y)))\n",
    "\n",
    "print(\"SK-Learn_LineReg  MY_MSE：{}\".format(MSE(line_reg.predict(X_test_std), y_test)))\n",
    "print(\"Scratch_LineReg  MY_MSE：{}\".format(MSE(y_test, predict_y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】学習曲線のプロット \n",
    "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "      \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      学習用データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証用データに対する損失の記録\n",
    "    self.loss_list : list, []\n",
    "      学習用データのイテレーション毎の損失関数リスト\n",
    "    self.loss_list : list, []\n",
    "      検証用データのイテレーション毎の損失関数リスト\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, bias, verbose):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        self.loss_list = []\n",
    "        self.loss_val_list = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証用データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            学習用データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.val : int64\n",
    "          検証用データの有無を識別。　0：無  1：有\n",
    "        self.Θj : 次の形のndarray, shape (n_samples, )\n",
    "          学習用データのΘ\n",
    "        self.Θj_val : 次の形のndarray, shape (n_samples, )\n",
    "          検証用データのΘ\n",
    "        \"\"\"\n",
    "        self.X_ = np.concatenate([np.ones((X.shape[0],1)), X], axis=1)\n",
    "        self.y_ = y\n",
    "        self.val = 0\n",
    "                \n",
    "        self.Θj = np.random.rand(1, self.X_.shape[1])\n",
    "\n",
    "\n",
    "        for iter_n in range(self.iter):\n",
    "            self._linear_hypothesis()\n",
    "            self._gradient_descent()\n",
    "            self._cost_function()\n",
    "            \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程を出力\n",
    "                print(\"ーー{}回目y~ーーーー\\n{}\".format(iter_n+1, self.h))\n",
    "                print(\"ーー{}回目Θーーーー\\n{}\".format(iter_n+2, self.Θj))\n",
    "        \n",
    "        if (X_val is not None and y_val is not None):\n",
    "            self.val = 1\n",
    "            self.X_val_ = np.concatenate([np.ones((X_val.shape[0],1)), X_val], axis=1)\n",
    "            self.y_val_ = y_val\n",
    "            self.Θj_val = np.random.rand(1, self.X_val_.shape[1])\n",
    "\n",
    "            for iter_n in range(self.iter):\n",
    "                self._linear_hypothesis_val()\n",
    "                self._gradient_descent_val()\n",
    "                self._cost_function_val()\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(\"ーー{}回目y~ーーーー\\n{}\".format(iter_n+1, self.h_val))\n",
    "                    print(\"ーー{}回目Θーーーー\\n{}\".format(iter_n+2, self.Θj_val))\n",
    "        \n",
    "    def _linear_hypothesis(self):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する    \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.h : 次の形のndarray, shape (n_samples, )\n",
    "          学習用データの仮定関数の計算結果\n",
    "        \"\"\"\n",
    "        self.h = np.dot(self.X_, self.Θj.T)\n",
    "    def _linear_hypothesis_val(self):\n",
    "        \"\"\"\n",
    "        検証データ用の_linear_hypothesis()     \n",
    "        Attributes\n",
    "        ----------\n",
    "        self.h_val : 次の形のndarray, shape (n_samples, )\n",
    "          学習用データの仮定関数の計算結果\n",
    "        \"\"\"\n",
    "        self.h_val = np.dot(self.X_val_, self.Θj_val.T)\n",
    "            \n",
    "    def _cost_function(self):\n",
    "        \"\"\"\n",
    "        線形の目的関数により、損失を算出する。(学習データ専用)\n",
    "        Parameters\n",
    "        ----------\n",
    "        JΘ : float64\n",
    "            目的関数による、損失計算のうち、サンプル数で割る前の段階の値\n",
    "        \"\"\"\n",
    "        JΘ =  ((self.h - self.y_)**2).sum()\n",
    "        self.loss = JΘ/(self.y_.shape[0]*2)  \n",
    "        self.loss_list.append(self.loss)\n",
    "    def _cost_function_val(self):\n",
    "        \"\"\"\n",
    "        検証データ用の _cost_function()     \n",
    "        Parameters\n",
    "        ----------\n",
    "        JΘ_val : float64\n",
    "            目的関数による、損失計算のうち、サンプル数で割る前の段階の値\n",
    "        \"\"\"        JΘ_val = ((self.h_val - self.y_val_)**2).sum()\n",
    "        self.loss_val = JΘ_val/(self.y_val_.shape[0]*2)        \n",
    "        self.loss_val_list.append(self.loss_val)\n",
    "\n",
    "    def _gradient_descent(self):\n",
    "        \"\"\"\n",
    "        再急降下法により、次回イテレーションのΘを算出する。(学習データ専用)\n",
    "        Parameters\n",
    "        ----------\n",
    "        Δj : 次の形のndarray, shape (n_samples, )\n",
    "            現状のΘにおける勾配\n",
    "        Attributes\n",
    "        ----------\n",
    "        self.Θj : 次の形のndarray, shape (n_samples, )\n",
    "          次回イテレーションのΘ\n",
    "        \"\"\"\n",
    "        Δj =  np.dot((self.h - self.y_).T , self.X_)\n",
    "        self.Θj -= self.lr*Δj\n",
    "    def _gradient_descent_val(self):\n",
    "        \"\"\"\n",
    "        検証データ用の _gradient_descent()     \n",
    "        Parameters\n",
    "        ----------\n",
    "        Δj_val : 次の形のndarray, shape (n_samples, )\n",
    "            現状のΘにおける勾配\n",
    "        Attributes\n",
    "        ----------\n",
    "        self.Θj_val : 次の形のndarray, shape (n_samples, )\n",
    "          次回イテレーションのΘ\n",
    "        \"\"\"\n",
    "        Δj_val =  np.dot((self.h_val - self.y_val_).T , self.X_val_)\n",
    "        self.Θj_val -= self.lr*Δj_val\n",
    "\n",
    "    def graph_cost_func(self):\n",
    "        \"\"\"\n",
    "        損失の推移をグラフ化する。    \n",
    "        検証用データが入力されていれば、学習用と検証用の損失推移を重ねてグラフ化\n",
    "        \"\"\"\n",
    "        plt.title(\"Num_of_Iteration vs Loss\")\n",
    "        plt.xlabel(\"Num_of_Iteration\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        a=range(self.iter)\n",
    "        plt.plot(range(1,self.iter+1), self.loss_list, color=\"b\", marker=\"o\", label=\"train_loss\")\n",
    "        if self.val is 1:\n",
    "            plt.plot(range(1,self.iter+1), self.loss_val_list, color=\"g\", marker=\"+\", label=\"val_loss\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        X = np.concatenate([np.ones((X.shape[0],1)), X], axis=1)\n",
    "        self.h = np.dot(X, self.Θj.T)\n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_line_reg = ScratchLinearRegression(200, 0.0001, False, False)\n",
    "scr_line_reg.fit(X_train_std, y_train, X_test_std, y_test)\n",
    "scr_line_reg.graph_cost_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_line_reg.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
